[{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Data Protection and Security Best Practices with Veeam on AWS Authors: Desmond Lai Xu and Vishwajeeth Venkatesh | March 11, 2025 | on Best Practices, Intermediate (200), Partner solutions, Security \u0026amp; Governance, Security, Identity, \u0026amp; Compliance, Storage | Permalink | Comments | Share\nSafeguarding sensitive information is crucial for all organizations. With the rise in data volumes from hundreds of terabytes to petabytes, and the inclusion of sensitive Personally Identifiable Information (PII), organizations are required to meet stringent regulatory requirements with respect to data security. Organizations need to protect data and their IT landscape from misconfigurations, human errors, and evolving cyber threats such as ransomware and vulnerabilities in environments. This leaves organizations susceptible to financial, reputational, and operational damage. Common customer challenges include:\nInsecure data backups: Poorly configured backups leave gaps in your disaster recovery plan. Data in transit vulnerabilities: Improper encryption during transmission opens data up to interception. IAM mismanagement: Due to identity and access misconfigurations. Ransomware targeting backups: Typically in ransomware attacks, backup repositories are the first target. These concerns highlight the critical need for robust data protection and effective cyber-resiliency strategies to mitigate risks and ensure compliance.\nData Protection Security Best Practices Overview When adopting AWS cloud, understanding the AWS Shared Responsibility Model is crucial. This model delineates the division of security responsibilities between AWS and the customer. While AWS manages the security of the cloud infrastructure, customers are responsible for securing their data within the cloud. This includes implementing appropriate identity and access management, encryption, and network security measures. Best practices in securing your data includes:\nImplement multi-layered and hardened backup strategies: Ensure backups are encrypted and logically isolated from public internet avoiding exposure and risks. Encrypt data in transit and at rest: Apply standards for transit and at-rest data. Strengthen IAM protocols: Implement least privilege, multi-factor authentication (MFA), and regular audits. Ransomware protection and backup isolation: Implement immutable repositories and isolate backup data. Veeam assists customers comply with regional, local, and industry regulations and frameworks including (Cloud Act, HIPAA, NIST, IRAP, MTCS Tier 3, OSPAR, ISO 20000, and more). Veeam implements this by seamlessly integrating with AWS-provided mechanisms and out-of-the-box solutions, enabling customers to effectively secure their AWS cloud environments.\nVeeam Backup on AWS Implementation \u0026amp; Configuration To ensure customer data remains secure and protected against potential risks, it is crucial to align Veeam deployments on AWS with established security best practices.\nIn the following sections, we will explore how Veeam integrates these best practices, to mitigate security threats, and ensure compliance with industry standards and reduce customer risk profile.\n1. Repository Immutability Store data in a state that cannot be modified after creation. This ensures data integrity and durability to meet audit and compliance requirements by preserving historical records and transactions ensuring data cannot be deleted or overwritten during a specific retention timeframe.\nImplementation and Integration by Veeam\nVeeam Backup for AWS allows you to protect data stored in backup repositories from deletion by making the data immutable until it reaches its desired retention period.\nVeeam Backup uses Amazon Simple Storage Service (S3) Object Lock to prevent backup data deletion or modification based on retention policies. In compliance mode, data cannot be tampered or deleted by any user including the AWS account root user, protecting against ransomware and malicious actions. For configuration details, refer to Immutability configuration guide on Veeam documentations.\n2. Reducing Blast Radius / Isolation Physically and logically isolating data, infrastructure, and applications minimizes the impact of failures. By compartmentalizing data and workloads, organizations can better segment access permissions and reduce the attack surface, making it harder for attackers to move laterally. Isolated environments allow organizations to pinpoint the source of a problem more quickly and apply targeted solutions without affecting other parts of the system.\nImplementation and Integration by Veeam\nVeeam allows you to create a separate backup account in which all the backup infrastructure can be deployed.\nThis can also be deployed in a separate AWS Region for further redundancy. This ensures availability if your AWS account gets compromised and addresses any issues in geo-level availability.\n3. Encryption Everywhere Protecting data in transit and at rest makes it unreadable to bad actors both internally and externally. Many regulatory standards and compliance frameworks require data encryption to safeguard sensitive information. By encrypting data in accordance with these requirements, organizations can ensure compliance and avoid potential penalties or legal issues.\nHaving encrypted backups and snapshots in a separate account and region allows customers to follow best practices in data protection. Veeam automatically deploys worker instances in production or backup accounts and removes it immediately after restore and backup processes are complete. These can be deployed separately from Appliance accounts where Veeam Backup for AWS is installed and Backup Repository accounts where backup data is stored in Amazon S3.\nImplementation and Integration by Veeam\nAmazon S3 buckets are encrypted by default using Amazon S3-managed keys (SSE-S3), providing foundational data security. For additional protection, Veeam Backup for AWS allows users to encrypt backup data stored in repositories through Veeam\u0026rsquo;s own encryption mechanisms.\nFurthermore, Veeam extends this security by supporting native AWS Key Management Service (AWS KMS) encryption for Amazon Elastic Cloud Compute (Amazon EC2) and Amazon Relation Database Service (Amazon RDS) instance volumes, Amazon Elastic File System (Amazon EFS), Amazon FSx file systems, Amazon DynamoDB tables, and cloud-native snapshots. Veeam uses the 256-bit Advanced Encryption Standard (AES) for its encryption process, ensuring robust protection.\nVeeam Backup for AWS uses AWS KMS keys and CMKs to encrypt backup data at rest and in transit. The encryption process secures snapshots at the block level before mounting to worker instances and maintains protection throughout the S3 backup workflow, ensuring data security and compliance.\nVeeam integrates seamlessly with AWS Identity and Access Management (IAM) roles, ensuring that only authorized roles can access or manage encrypted backups. By enforcing encryption across all data points, Veeam simplifies managing encryption policies while bolstering the overall security of backup data.\n4. Identity and Access Management Correct configuration of IAM ensures the right users access the right data at the right time, with minimal levels of permission to perform a task. This provides centralized control and visibility into user access and activity across AWS services and resources, enabling organizations to monitor user behavior, detect suspicious activity, and respond to security incidents in real-time.\nImplementation and Integration by Veeam\nVeeam allows you to create granular IAM roles to perform backup and restore operations in multiple AWS accounts. Veeam enforces least privileged access, giving users and systems only the minimum level of access needed to perform only their necessary tasks, reducing the potential damage in the event of a compromise.\nVeeam integrates with single sign-on providers and multi-factor authentication for enhanced security. Granular role assignments reduce compromise risks during AWS security audits. For details, refer to Veeam Backup for AWS IAM Permissions documentation for more information.\n5. End-to-End Private Connectivity Private network deployment reduces security risks by preventing public internet exposure, protecting against eavesdropping and interception while improving network performance and reliability.\nDedicated connections or VPNs provide predictable bandwidth, lower latency, and higher throughput, ensuring optimal performance for mission-critical applications and workloads.\nImplementation and Integration by Veeam\nVeeam supports private deployment of backup infrastructure to ensure core backup components are secure and do not have public-facing endpoints. Veeam allows you to deploy backup appliance in a private environment.\nAdditionally, Veeam can enable private network deployment functionality, allowing communication to Amazon S3 via private Amazon S3 interface endpoints. Veeam allows you to deploy workers in private environments without public IPV4 assignment which ensures backup traffic flow is secure.\nIf you are looking to deploy Veeam Backup for AWS in a private environment and need guidance, Veeam provides an automation script to help you get started.\nConclusion Veeam®, the global market leader in data protection and ransomware recovery, is on a mission to empower organizations to not just bounce back from a data outage or loss but to bounce forward. With Veeam, organizations achieve radical resilience through data security, data recovery, and data freedom for their hybrid cloud environments. The Veeam Data Platform delivers a single solution for cloud, virtual, physical, SaaS, and Kubernetes environments, giving IT and security leaders peace of mind that their apps and data are protected and always available. Headquartered in Columbus, Ohio, with offices in more than 30 countries, Veeam protects over 450,000 customers worldwide, including 73% of the Global 2000, who trust Veeam to keep their businesses running.\nVeeam on AWS provides comprehensive data protection and ransomware recovery through security best practices, enhanced by AWS\u0026rsquo;s scalable infrastructure for backup and disaster recovery capabilities.\nThe Veeam-AWS partnership leverages S3 and Glacier Deep Archive for secure, cost-effective data protection and rapid recovery. Following AWS best practices, Veeam ensures data security, resilience, and regulatory compliance while protecting against ransomware and data loss.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Effectively implementing resource control policies in a multi-account environment Authors: Tatyana Yatskevich and Harsha Sharma | March 26, 2025 | on Announcements, Intermediate (200), Security, Identity, \u0026amp; Compliance, Technical How-to | Permalink | Comments | Share\nEvery organization strives to empower teams to drive innovation while safeguarding their data and systems from unintended access. For organizations that have thousands of Amazon Web Services (AWS) resources spread across multiple accounts, organization-wide permissions guardrails can help maintain secure and compliant configurations. For example, some AWS services support resource-based policies that can be used to grant identities permissions to perform actions on the resources they\u0026rsquo;re attached to. With the management of resource-based policies frequently delegated to application owners, central security teams use permissions guardrails to help ensure that possible misconfigurations don\u0026rsquo;t lead to unintended access to these resources.\nIn this post, we discuss how you can use resource control policies (RCPs) to centrally restrict access to resources. We demonstrate how RCPs can help improve your security posture while allowing even more freedom to developers in managing their resources, thus reducing friction between central security and application teams. Using a sample use case, we uncover key considerations for designing and effectively implementing RCPs in your organization at scale.\nIf you\u0026rsquo;re new to RCPs, we recommend starting with Introducing resource control policies (RCPs), a new type of authorization policy in AWS Organizations, which provides an introduction to RCPs and their role in your security strategy.\nRCP implementation journey RCPs are a type of authorization policy in AWS Organizations. RCPs work alongside service control policies (SCPs) to help establish permissions guardrails across multiple accounts in your organization. To understand their differences and use cases, see General use cases for SCPs and RCPs and Enforcing enterprise-wide preventive controls with AWS Organizations.\nWe recommend implementing permissions guardrails, including RCPs, using the following iterative process, which consists of five phases (as shown in Figure 1).\nExamine your security control objectives Design permissions guardrails Anticipate potential impacts Implement permissions guardrails Monitor permissions guardrails Figure 1: Permissions guardrails implementation journey\nThis phased approach helps ensure an effective integration of RCPs into your security strategy, improving your security posture while helping to maintain business continuity. Let\u0026rsquo;s explore each phase of RCP implementation in detail and outline key considerations for an effective implementation strategy.\nPhase 1: Examine your security control objectives The first step in implementing RCPs is identifying areas where RCPs can help improve your security posture or optimize the implementation of controls for your organization\u0026rsquo;s specific security control objectives.\nYour control objectives can be influenced by a variety of factors such as compliance and regulatory requirements, legal and contractual obligations, types of workloads, data classification, and your organization\u0026rsquo;s threat model. After your control objectives are well-defined and prioritized, identify those that can be achieved using RCPs.\nLike SCPs, RCPs are designed to establish coarse-grained access controls, security invariants that rarely change and serve as always-on boundaries across a wide range of AWS resources in your accounts. RCPs aren\u0026rsquo;t for managing fine-grained access controls. You will keep using policies such as resource-based and identity-based policies to apply least-privilege permissions.\nMore specifically, the following are key control objectives that you can achieve using RCPs:\nEstablish a data perimeter around your AWS resources. For example, you can use RCPs to help ensure that only trusted identities can access your AWS resources. Mitigate the cross-service confused deputy risk. You can use RCPs to help ensure that your AWS resources are accessed by AWS services only on behalf of your organization. Apply consistent access controls to your AWS resources regardless of the identities accessing them. For example, you can use RCPs to help ensure your Amazon Simple Storage Service (Amazon S3) buckets require TLS v1.2 or higher for in-transit encryption. For additional use cases and types of controls that can be implemented using RCPs, you can explore the resource control policy examples repository. In this post, we demonstrate how to help ensure that only trusted identities can access your AWS Identity and Access Management (IAM) roles.\nLet\u0026rsquo;s begin with the scenario illustrated in Figure 2. Your company\u0026rsquo;s central cloud team manages your corporate AWS Organizations organization, which consists of two corporate AWS accounts. An IAM principal in Account A should be able to assume an IAM role in Account B to perform day-to-day operations. To align to the broader control objective of Only trusted identities can access my resources, the central security team wants to make sure that the IAM role in Account B (my resource) can only be assumed by IAM principals that belong to their organization (trusted identities).\nFigure 2: Simple scenario depicting a trusted identity accessing an IAM role\nOne way of achieving this control objective is to follow the principle of least-privilege and make sure that the role trust policy, the resource-based policy attached to the IAM role, only allows access to identities that require that access. The following is an example trust policy that grants permissions to Role A in Account A to assume Role B in Account B.\nIn organizations that have only a few accounts, central teams typically manage these policies. While this centralized governance model helps ensure that trust policies applied to roles are always restricted to trusted identities, it can also impede the productivity of application teams when operating at a greater scale.\nAssume that your company has started growing its cloud footprint so much that your central security team now must achieve the same control objective with hundreds of IAM roles that are spread across multiple AWS accounts, as demonstrated in Figure 3.\nFigure 3: Restricting access by managing individual IAM role trust policies\nAt this scale, we see organizations delegating permissions management to application teams to better support the growth of their business and empower developers to innovate faster. While central security teams no longer have full control over the permissions granted to resources across AWS accounts, they must make sure that access is aligned with their organization\u0026rsquo;s security standard. For example, they might want to make sure that the GrantCrossAccountAccess statement that is now managed by developers doesn\u0026rsquo;t inadvertently grant access to an account that doesn\u0026rsquo;t belong to their organization. Previously, central security teams typically achieved this by developing automated mechanisms to insert a standard statement into all trust policies. This statement helped ensure that access remained bounded to their organization, even when developers configured broad access permissions for their roles. The following is an example trust policy where a developer granted permissions to an external account through the GrantCrossAccountAccess statement. However, because of the RestrictAccessToMyOrg statement added to the policy by the central security team, the external account will be unable to use these permissions.\nThe RestrictAccessToMyOrg statement uses the aws:PrincipalOrgID and aws:PrincipalIsAWSService condition keys to restrict access to principals within your organization or to AWS service principals. The BoolIfExists operator with the aws:PrincipalIsAWSService condition key is required if the roles you\u0026rsquo;re applying a control to are service roles that are used by AWS services to perform operations on your behalf. When an AWS service assumes a service role, it uses its AWS service principal, an identity that is owned by AWS and that does not belong to your organization.\nThe central security teams could, for example, use AWS Config rules to detect misconfigurations and then use AWS Config remediation to automatically add the RestrictAccessToMyOrg statement to the IAM roles\u0026rsquo; trust policies when new IAM roles are created or their trust policies are changed. Even though the addition of the RestrictAccessToMyOrg statement to trust policies can be automated, RCPs can greatly simplify enforcement of such coarse-grained controls in a multi-account environment.\nPhase 2: Design permissions guardrails After you have identified the use case you want to address and the entities or resources that you want to apply controls for, you can begin designing your RCPs. If you\u0026rsquo;re new to RCPs, we strongly advise that you start your implementation journey with one use case at a time so that you can incrementally build your experience and fine-tune your controls. For the rest of this post, we will walk you through implementing an RCP that limits all of your IAM roles to being assumed by identities within your trusted perimeter. Specifically, we will examine a use case where we want to make sure that the roles in our organization can only be assumed by identities within our organization, by AWS service principals, or by a set of trusted partners who have a valid business reason to assume the roles.\nDetermine an applicable attachment target\nAfter you have determined the control that you want to use, you need to identify the target for the control attachment to minimize the scope of impact of your RCP. RCPs currently support attachment at the root, OU, or account level. The root and OU levels provide an effective way of centralizing control management and applying controls at scale without the need to perform operations on individual accounts.\nIf you choose to implement the control at the OU level and you organize your accounts into OUs based on the compliance requirements applicable to them or based on a common set of controls that govern those accounts, then you can create and deploy multiple RCPs for different use cases as applicable to the sets of accounts organized within OUs. For example, as shown in Figure 2, you might have one OU for accounts that store critical data and another for the accounts that store non-critical data. You can use dedicated RCPs for these OUs to make sure that the security requirements for each specific type of data are met. Additionally, if you choose to enforce data residency using Identity and Access Management Access Analyzer, you can create a dedicated OU for organizing the accounts that are subject to these requirements and apply an RCP that helps prevent creation of multi-region access points (MRAPs) for any Amazon S3 buckets in this OU, thus reducing the possibility of unintentional cross-region replication.\nIf you choose to implement the control at the root or OU levels, it\u0026rsquo;s important to be aware that the RCP applies to all resources under the specified entity. Make sure that the control is designed correctly so that it doesn\u0026rsquo;t impact resources that shouldn\u0026rsquo;t be subject to the control.\nCentral security teams can implement permissions guardrails by creating an RCP that centrally blocks external access to IAM roles. The RCP that you will implement contains similar restrictions to the RestrictAccessToMyOrg statement that you used in the IAM trust policy.\nLike SCPs, you attach the RCP to an account, organizational unit (OU), or the root of your organization. After being attached, the RCP automatically applies to applicable resources—in this case, IAM roles—within the scope of that AWS Organizations entity. This centralized approach alleviates the need to modify hundreds of trust policies across multiple accounts, lowering the operational overhead for central security teams and helping ensure consistent access controls are applied at scale. RCPs also help you achieve separation of duties with developers still managing their least-privilege permissions in trust policies and administrators applying coarse-grained access controls in RCPs. If developers make configuration mistakes while managing permissions for their applications, the preventative access controls implemented using RCPs will help ensure that they stay within your organization\u0026rsquo;s access control guidelines. See How AWS enforcement code logic evaluates requests to allow or deny access to understand how different policy types impact the authorization process.\nIf you\u0026rsquo;re transitioning existing controls from resource-based policies to RCPs, use the opportunity to reassess the control design based on your current control objectives and the additional benefits offered by RCPs. For example, your previous controls might have been limited to specific resource types, such as IAM roles in this use case, or to particular accounts, such as those storing the most sensitive data. RCPs enable you to extend controls to additional resources across your entire organization, reducing operational overhead through centralized management of permissions guardrails.\nIf you need to apply a control on resources not yet covered by RCPs, you can implement or retain your custom automation for enforcing controls with resource-based policies. See the List of AWS services that support RCPs and Resources and entities not restricted by RCPs and plan for additional controls if applicable.\nWhile designing your RCPs, consider the following guidelines.\nDesign for operational excellence\nA key foundation for effectively implementing and operating permissions guardrails like RCPs is organizing your AWS environment using multiple accounts. Account boundaries and strategic placement of workloads across them allow you to apply tailored access controls that align with data sensitivity and specific access requirements. Grouping accounts into OUs within AWS Organizations enables more effective access control, even in scenarios where cross-account access is required. Figure 4 illustrates an example organization structure, demonstrating how RCPs can be applied at various levels of the organizational hierarchy to adhere to the security requirements of different workloads.\nFigure 4: A sample organization with RCPs applied at various levels\nWhen operating at scale, consider delegating policy management to a central security account in your organization. With AWS Organizations resource-based delegation, central teams don\u0026rsquo;t need access to the management account for any SCP or RCP related changes or troubleshooting.\nReview Achieving operational excellence with design considerations for AWS Organizations SCPs, which focuses on SCPs but also covers foundational principles for designing and implementing permissions guardrails at scale. These considerations also apply to RCPs for enabling operational excellence. Additionally, see AWS Organizations quotas and RCP evaluation for the RCP-related quotas and unique implementation details.\nDefine your governance\nEstablishing clear governance helps you define how to implement and continuously manage RCPs within your organization. This includes the operating model, change management processes, and exceptions handling procedures. RCPs provide authorization controls similar to SCPs and therefore should integrate with your existing governance framework rather than requiring separate oversight. For example, if your change management process requires two-person approval for SCP changes, you should consider applying the same approval process for RCP implementation. You should also adopt the same mechanisms you currently use to prevent unauthorized changes or detect drifts in your policies.\nPlan for exceptions\nThere might be scenarios where you have a few resources that should be accessible publicly or by identities that don\u0026rsquo;t belong to your organization. If you\u0026rsquo;re organizing your resources across multiple accounts and OUs based on their compliance requirements or a common set of controls, then you most likely have such resources in a dedicated set of accounts or OUs, such as the Public Data OU in Figure 4. These accounts or OUs can have applicable policies that account for their unique access requirements.\nAnother option to accommodate these scenarios is to use the aws:ResourceAccount or aws:ResourceOrgPaths condition key to exclude certain accounts from the control. For example, the following policy will deny access to identities outside your organization from assuming IAM roles unless the identity is an AWS service principal or the role that is being accessed belongs to Account A.\nThere also might be situations where your company\u0026rsquo;s trusted partners or acquisitions need to be granted an exception for access to a subset of your company\u0026rsquo;s resources distributed across multiple accounts. For example, your company might integrate with Cloud Security Posture Management (CSPM) tools that assume roles in your accounts to assess your accounts\u0026rsquo; security posture, as shown in Figure 5.\nFigure 5: Representative view of granting exceptions to trusted partners\nWhen implementing a control with an RCP that by default will apply to all resources of the entity it\u0026rsquo;s attached to, you can manage resource specific exceptions using the aws:ResourceTag condition key. In addition, use the aws:PrincipalAccount context key to conditionally grant exceptions based on the AWS account ID of the trusted partner.\nLet\u0026rsquo;s examine the two statements in the preceding RCP:\nRestrictAccessToMyOrgExceptTaggedRoles\nThis statement helps ensure that your roles can only be assumed by identities that belong to your organization or by AWS service principals, unless a role is tagged with partner-access-exception set to trusted-partner. RestrictAccessForTaggedRoles\nThis statement further restricts access by helping ensure that the roles that have the partner-access-exception tag can only be assumed by identities that belong to your trusted partner account. If you have a well-known, tightly scoped set of resources that need to be excluded, you can also use the IAM policy element, NotResource, to list the Amazon Resource Names (ARNs) of resources to exclude from the control.\nWhen implementing tag-based exception processes, establishing strict controls over tag management is key. Unauthorized modifications of tags on resources, principals, or sessions could impact your security posture by enabling unintended access. You should implement controls to help prevent unauthorized tag manipulation. For example, the following SCP restricts the use of the partner-access-exception tag to the admin role so that unauthorized users cannot alter the control by attaching, detaching, or modifying the tag.\nYou should also make sure that the partner-access-exception tag cannot be passed as a session tag when identities assume roles. See the sample RCP in the data perimeter policy examples repository.\nPhase 3: Anticipate potential impacts Before rolling out RCPs, you need to understand their potential impact on your organization. Introducing new policies or modifying existing ones without proper validation can disrupt your security-productivity balance. Be aware that overly restrictive policies might inadvertently impede legitimate data flows that are essential for achieving your business objectives.\nConsider using AWS Identity and Access Management Access Analyzer to monitor effective permissions across resources in your organization. For our IAM role example, use an organization external access analyzer to identify IAM roles in your organization that are shared with external entities. This analysis will help you to create appropriate exceptions or lock down any overly permissive access.\nAnother effective method to assess impact is to review and analyze your account activity using AWS CloudTrail. For example, if you centralize all your CloudTrail logs in an S3 bucket, you can use Amazon Athena to query these logs. Specifically, look for STS API calls made against your IAM roles by identities outside your organization. Then, compare the results with your list of known trusted partners and those you have already accounted for in your RCPs. Based on this analysis, determine if you need to add the partner-access-exception tag to additional IAM roles and further refine the policy before enforcement. This is essential to ensure trusted partner integrations continue to function as expected when you enforce your RCPs. Furthermore, use this analysis to identify any illegitimate access patterns in your environment and plan for necessary remediations, further enhancing your security posture as part of RCP implementation.\nFor detailed guidance on how to perform an impact analysis in your environment, see Analyze your account activity to evaluate impact and refine controls, which describes the tools and options you need to be able to conduct the analysis.\nPhase 4: Implement permissions guardrails As you transition into the implementation phase, consider the following key factors to promote a smooth rollout while enhancing your security posture.\nDeployment automation and integration\nUse your existing deployment pipelines to implement RCPs, the same as you do for SCPs. This approach will minimize operational overhead while maintaining consistency in the deployment of your controls.\nYou can use the AWS CloudFormation AWS::Organizations::Policy resource type to deploy RCPs as infrastructure as code (IaC) using your continuous integration and continuous delivery (CI/CD) pipeline. If you\u0026rsquo;re using AWS Control Tower and the Customizations for AWS Control Tower solution (CfCT) for account management and want to deploy your custom RCPs, use rcp as the deploy_method in the CfCT manifest file. You can also take advantage of the AWS Control Tower provided RCP-based controls to streamline the implementation.\nProgressive deployment in stages\nAs with SCPs, AWS strongly advises against attaching RCPs in production environments without thoroughly testing the impact that the policies have on resources in your accounts. Follow standard CI/CD processes and begin your RCP rollout in lower environments by attaching them to individual test accounts or OUs first. After you validate that the controls behave as excepted, gradually promote the RCPs to upper environments.\nIf your goal is to transition an existing control from resource-based policies to RCPs, keep your resource-based policies in place while conducting the progressive rollout. After you have completed rolling out your RCPs and confirmed that they operate as expected, you can consider deactivating the automation you used to apply the control using resource-based policies. This approach lets you deploy RCPs without impacting your existing security posture or disrupting business workflows.\nAdditionally, consider deploying RCPs to a subset of resources or accounts first to limit the scope of impact and provide an opportunity to test and refine your deployment and operational processes. You can follow your standard prioritization approach to define deployment waves, for example, start with resources or accounts that store sensitive data or pose the highest risk, based on your current operational practices and other controls that might be in place. For additional best practices, see OPS06-BP03 Employ safe deployment strategies in the AWS Well-Architected Framework: Operation Excellence Pillar whitepaper.\nPhase 5: Monitor permissions guardrails Finally, establish monitoring processes to help ensure that controls for preventing external access to your resources operate as expected. You can use the same tools you used for impact analysis. For example, you can use IAM Access Analyzer external access findings to understand the impact of your RCPs on resource permissions. This information will help you verify that your RCPs are crafted in accordance with your intent and plan remediation actions, if required. You can also set alerts for occurrences of unintended access patterns observed in your CloudTrail logs.\nFurthermore, follow the phased approach outlined in this post to regularly review and update your controls to help ensure that they align with evolving business and security objectives. Consider factors such as organizational changes, changes in partner relationships, data criticality shifts, and opportunities for expanding your RCP coverage. This continuous improvement process helps maintain the effectiveness of your security controls while supporting business growth and transformation.\nConclusion In this post, we discussed how to effectively implement coarse-grained access controls on AWS resources at scale using RCPs. You can use the phased implementation approach described here to achieve your security control objectives while minimizing the risk of disrupting your business workflows. You can apply the same approach to implement other preventative controls, such as SCPs, across your multi-account environment.\nRemember that RCPs, like SCPs, provide a powerful mechanism for enforcing coarse-grained controls across multiple accounts in your organization. They don\u0026rsquo;t replace your least-privilege controls and should be part of a broader, multi-layered approach to data security that includes other well-architected security design principles.\nIf you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Dynamic text-to-SQL for enterprise workloads with Amazon Bedrock Agents Authors: Jiwon Yeom and Jimin Kim | April 14, 2025 | Amazon Bedrock, Amazon Bedrock Agents, Best Practices, Generative AI, Technical How-to, Thought Leadership | Permalink | Comments | Share\nGenerative AI enables us to accomplish more in less time. Text-to-SQL empowers people to explore data and draw insights using natural language, without requiring specialized database knowledge. Amazon Web Services (AWS) has helped many customers connect this text-to-SQL capability with their own data, which means more employees can generate insights. In this process, we discovered that a different approach is needed in enterprise environments where there are over 100 tables, each with dozens of columns. We also learned that robust error handling is critical when errors occur in the generated SQL query based on users\u0026rsquo; questions.\nThis post demonstrates how enterprises can implement a scalable agentic text-to-SQL solution using Amazon Bedrock Agents, with advanced error-handling tools and automated schema discovery to enhance database query efficiency. Our agent-based solution offers two key strengths:\nAutomated scalable schema discovery – The schema and table metadata can be dynamically updated to generate SQL when the initial attempt to execute the query fails. This is important for enterprise customers who have a lot of tables and columns and many queries patterns. Automated error handling – The error message is directly fed back to the agent to improve the success rate of running queries. You\u0026rsquo;ll find that these features help you tackle enterprise-scale database challenges while making your text-to-SQL experience more robust and efficient.\nUse case An agentic text-to-SQL solution can benefit enterprises with complex data structures. In this post, to understand the mechanics and benefits of the agentic text-to-SQL solution in a complex enterprise environment, imagine you\u0026rsquo;re a business analyst on the risk management team in a bank. You need to answer questions such as \u0026ldquo;Find all transactions that occurred in the United States and were flagged as fraudulent, along with the device information used for those transactions,\u0026rdquo; or \u0026ldquo;Retrieve all transactions for John Doe that occurred between January 1, 2023, and December 31, 2023, including fraud flags and merchant details.\u0026rdquo; For this, there are dozens—or sometimes hundreds—of tables that you need to not only be aware but also craft complex JOIN queries. The following diagram illustrates a sample table schema that might be needed for fraud investigations.\nThe key pain points of implementing a text-to-SQL solution in this complex environment include the following, but aren\u0026rsquo;t limited to:\nThe amount of table information and schema will get excessive, which will entail manual updates on the prompts and limit its scale. As a result, the solution might require additional validation, impacting the quality and performance of generating SQL. Now, consider our solution and how it addresses these problems.\nSolution overview Amazon Bedrock Agents seamlessly manages the entire process from question interpretation to query execution and result interpretation, without manual intervention. It seamlessly incorporates multiple tools, and the agent analyzes and responds to unexpected results. When queries fail, the agent autonomously analyzes error messages, modifies queries, and retries—a key benefit over static systems.\nAs of December 2024, the Amazon Bedrock with structured data feature provides built-in support for Amazon Redshift, offering seamless text-to-SQL capabilities without custom implementation. This is recommended as the primary solution for Amazon Redshift users.\nHere are the capabilities that this solution offers:\nExecuting text-to-SQL with autonomous troubleshooting: The agent can interpret natural language questions and convert them into SQL queries. It then executes these queries against an Amazon Athena database and returns the results. If a query execution fails, the agent can analyze the error messages returned by AWS Lambda and automatically retries the modified query when appropriate. Dynamic schema discovery Listing tables – The agent can provide a comprehensive list of the tables in the fraud detection database. This helps users understand the available data structures. Describing table schemas – Users can request detailed information about the schema of specific tables. The agent will provide column names, data types, and associated comments, giving users a clear understanding of the data structure. The solution uses direct database tools for schema discovery instead of vector store–based retrieval or static schema definitions. This approach provides complete accuracy with lower operational overhead because it doesn\u0026rsquo;t require a synchronization mechanism and continually reflects the current database structure. Direct schema access through tools is more maintainable than hardcoded approaches that require manual updates, and it provides better performance and cost-efficiency through real-time database interaction.\nThe workflow is as follows:\nA user asks questions to Amazon Bedrock Agents. To serve the user\u0026rsquo;s questions, the agent determines the appropriate action to invoke: To execute the generated query with confidence, the agent will invoke the athena-query To confirm the database schema first, the agent will invoke the athena-schema-reader tool: Retrieve a list of available tables using its /list_tables endpoint. Obtain the specific schema of a certain table using its /describe_table endpoint. The Lambda function sends the query to Athena to execute. Athena queries the data from the Amazon Simple Storage Service (Amazon S3) data bucket and stores the query results in the S3 output bucket. The Lambda function retrieves and processes the results. If an error occurs: The Lambda function captures and formats the error message for the agent to understand. The error message is returned to Amazon Bedrock Agents. The agent analyzes the error message and tries to resolve it. To retry with the modified query, the agent may repeat steps 2–5. The agent formats and presents the final responses to the user. The following architecture diagram shows this workflow.\nImplementation walkthrough To implement the solution, use the instructions in the following sections.\nIntelligent error handling Our agentic text-to-SQL solution implements practical error handling that helps agents understand and recover from issues. By structuring errors with consistent elements, returning nonbreaking errors where possible, and providing contextual hints, the system enables agents to self-correct and continue their reasoning process.\nAgent instructions Consider the key prompt components that make this solution unique. Intelligent error handling helps automate troubleshooting and refine the query by letting the agent understand the type of errors and what to do when error happens:\nExecution and Error Handling: - Execute the query via the /athena_query endpoint - If the execution fails, carefully analyze the error message and hint provided by the Lambda function - Based on the error type received from the Lambda function, take appropriate action: - After identifying the issue based on the error message and hint: 1. Modify your query or API request to address the specific problem 2. If needed, use schema discovery tools (/list_tables, /describe_table) to gather updated information 3. Reconstruct the query with the necessary corrections 4. Retry the execution with the modified query or request The prompt gives guidance on how to approach the errors. It also states that the error types and hints will be provided by Lambda. In the next section, we explain how Lambda processes the errors and passes them to the agent.\nImplementation details Here are some key examples from our error handling system:\nERROR_MESSAGES = { \u0026#39;QUERY_EXECUTION_FAILED\u0026#39;: { \u0026#39;message\u0026#39;: \u0026#39;Failed to execute query\u0026#39;, \u0026#39;hint\u0026#39;: \u0026#39;Please use fully qualified table names. Example: SELECT * FROM fraud_data.customers LIMIT 1\u0026#39; }, \u0026#39;QUERY_RESULT_ERROR\u0026#39;: { \u0026#39;message\u0026#39;: \u0026#39;Error occurred while getting query results\u0026#39;, \u0026#39;hint\u0026#39;: \u0026#39;Check if the tables and columns in your query exist and you have proper permissions. Examples: \u0026#34;customers\u0026#34;, \u0026#34;transactions\u0026#34;, or \u0026#34;devices\u0026#34;.\u0026#39; }, \u0026#39;MISSING_QUERY\u0026#39;: { \u0026#39;message\u0026#39;: \u0026#39;Query is required\u0026#39;, \u0026#39;hint\u0026#39;: \u0026#39;No query was provided. Please provide a SQL query to execute\u0026#39; } } def create_query_response(query_result, status_code=200): if query_result.get(\u0026#39;error\u0026#39;): error_info = ERROR_MESSAGES.get(query_result[\u0026#39;error\u0026#39;]) return { \u0026#39;error\u0026#39;: query_result[\u0026#39;error\u0026#39;], \u0026#39;message\u0026#39;: error_info[\u0026#39;message\u0026#39;], \u0026#39;hint\u0026#39;: error_info[\u0026#39;hint\u0026#39;] } return query_result These error types cover the main scenarios in text-to-SQL interactions:\nQuery execution failures – Handles syntax errors and table reference issues, guiding the agent to use the correct table names and SQL syntax Result retrieval issues – Addresses permission problems and invalid column references, helping the agent verify the schema and access rights API validation – Verifies that basic requirements are met before query execution, minimizing unnecessary API calls Each error type includes both an explanatory message and an actionable hint, enabling the agent to take appropriate corrective steps. This implementation shows how straightforward it can be to enable intelligent error handling; instead of handling errors traditionally within Lambda, we return structured error messages that the agent can understand and act upon.\nDynamic schema discovery The schema discovery is pivotal to keeping Amazon Bedrock Agents consuming the most recent and relevant schema information.\nAgent instructions Instead of hardcoded database schema information, we allow the agent to discover the database schema dynamically. We\u0026rsquo;ve created two API endpoints for this purpose:\nSchema Discovery: - Use /list_tables endpoint to identify available tables in the database - Use /describe_table endpoint to get detailed schema information for specific tables - Always use the most recent and relevant table schemas, as the database structure may change frequently - Before constructing queries, ensure you have up-to-date schema information Implementation details Based on the agent instructions, the agent will invoke the appropriate API endpoint.\nThe /list_tables endpoint lists the tables in a specified database. This is particularly useful when you have multiple databases or frequently add new tables:\n@app.post(\u0026#34;/list_tables\u0026#34;, description=\u0026#34;Retrieve a list of all tables in the specified database\u0026#34;) def list_tables(event, database_name): query = f\u0026#34;SHOW TABLES IN {database_name}\u0026#34; result = execute_and_get_results(query, s3_output) if isinstance(result, dict) and \u0026#39;error\u0026#39; in result: return create_api_response(event, 400, get_error_response(\u0026#39;QUERY_RESULT_ERROR\u0026#39;)) return create_api_response(event, 200, result) The /describe_table endpoint reads a specific table\u0026rsquo;s schema with details. We use the \u0026ldquo;DESCRIBE\u0026rdquo; command, which includes column comments along with other schema details. These comments help the agent better understand the meaning of the individual columns:\n@app.post(\u0026#34;/describe_table\u0026#34;, description=\u0026#34;Retrieve the schema information of a specific table\u0026#34;) def describe_table(event, database_name, table_name): query = f\u0026#34;DESCRIBE {database_name}.{table_name}\u0026#34; result = execute_and_get_results(query, s3_output) if isinstance(result, dict) and \u0026#39;error\u0026#39; in result: return create_api_response(event, 400, get_error_response(\u0026#39;QUERY_RESULT_ERROR\u0026#39;)) formatted_result = { \u0026#34;table_name\u0026#34;: table_name, \u0026#34;database\u0026#34;: database_name, \u0026#34;columns\u0026#34;: result } return create_api_response(event, 200, formatted_result) When implementing a dynamic schema reader, consider including comprehensive column descriptions to enhance the agent\u0026rsquo;s understanding of the data model.\nThese endpoints enable the agent to maintain an up-to-date understanding of the database structure, improving its ability to generate accurate queries and adapt to changes in the schema.\nDemonstration You might not experience the exact same response with the presented screenshot due to the indeterministic nature of large language models (LLMs).\nThe solution is available for you to deploy in your environment with sample data. Clone the repository from this GitHub link and follow the README guidance. After you deploy the two stacks—AwsText2Sql-DbStack and AwsText2Sql-AgentStack—follow these steps to put the solution in action:\nGo to Amazon Bedrock and select Agents. Select AwsText-to-SQL-AgentStack-DynamicAgent and test by asking questions in the Test window on the right. Example interactions: Which demographic groups or industries are most frequently targeted by fraudsters? Present aggregated data. What specific methods or techniques are commonly used by perpetrators in the reported fraud cases? What patterns or trends can we identify in the timing and location of fraud incidents? Show the details of customers who have made transactions with merchants located in Denver. Provide a list of all merchants along with the total number of transactions they\u0026rsquo;ve processed and the number of those transactions that were flagged as fraudulent. List the top five customers based on the highest transaction amounts they\u0026rsquo;ve made. Choose Show trace and examine each step to understand what tools are used and the agent\u0026rsquo;s rationale for approaching your question, as shown in the following screenshot. (Optional) You can test the Amazon Bedrock Agents code interpreter by enabling it in Agent settings. Follow the instructions at Enable code interpretation in Amazon Bedrock and ask the agent \u0026ldquo;Create a bar chart showing the top three cities that have the most fraud cases.\u0026rdquo; Best practices Building on our discussion of dynamic schema discovery and intelligent error handling, here are key practices to optimize your agentic text-to-SQL solution:\nUse dynamic schema discovery and error handling – Use endpoints such as /list_tables and /describe_table to allow the agent to dynamically adapt to your database structure. Implement comprehensive error handling as demonstrated earlier, enabling the agent to interpret and respond to various error types effectively. Balance static and dynamic information – Although dynamic discovery is powerful, consider including crucial, stable information in the prompt. This might include database names, key table relationships, or frequently used tables that rarely change. Striking this balance can improve performance without sacrificing flexibility. Tailoring to your environment – We designed the sample to always invoke /list_tables and /describe_table, and your implementation might need adjustments. Consider your specific database engine\u0026rsquo;s capabilities and limitations. You might need to provide additional context beyond only column comments. Think about including database descriptions, table relationships, or common query patterns. The key is to give your agent as much relevant information as possible about your data model and business context, whether through extended metadata, custom endpoints, or detailed instructions. Implement robust data protection – Although our solution uses Athena, which inherently doesn\u0026rsquo;t support write operations, it\u0026rsquo;s crucial to consider data protection in your specific environment. Start with clear instructions in the prompt (for example, \u0026ldquo;read-only operations only\u0026rdquo;), and consider additional layers such as Amazon Bedrock Guardrails or an LLM-based review system to make sure that generated queries align with your security policies. Implement layered authorization – To enhance data privacy when using Amazon Bedrock Agents, you can use services such as Amazon Verified Permissions to validate user access before the agent processes sensitive data. Pass user identity information, such as a JWT token, to the agent and its associated Lambda function, enabling fine-grained authorization checks against pre-built policies. By enforcing access control at the application level based on the Verified Permissions decision, you can mitigate unintended data disclosure and maintain strong data isolation. To learn more, refer to Enhancing data privacy with layered authorization for Amazon Bedrock Agents in the AWS Security Blog. Identify the best orchestration strategy for your agent – Amazon Bedrock provides you with an option to customize your agent\u0026rsquo;s orchestration strategy. Custom orchestration gives you full control of how you want your agents to handle multistep tasks, make decisions, and execute workflows. By implementing these practices, you can create a text-to-SQL solution that not only uses the full potential of AI agents, it also maintains the security and integrity of your data systems.\nConclusion In conclusion, the implementation of a scalable agentic text-to-SQL solution using AWS services offers significant advantages for enterprise workloads. By using automated schema discovery and robust error handling, organizations can efficiently manage complex databases with numerous tables and columns. The agent-based approach promotes dynamic query generation and refinement, leading to higher success rates in data querying. We\u0026rsquo;d like to invite you to try this solution out today! Visit GitHub to dive deeper into the details of the solution, and follow the deployment guide to test in your AWS account.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://rinnn465.github.io/aws-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Takeaways: \u0026ldquo;AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\u0026rdquo; Program Focus Explore the full landscape of AI/ML/GenAI capabilities on AWS and methodologies for applying them to real-world scenarios. Speakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha – Community Builder Key Highlights 1. Generative AI with Amazon Bedrock Platform Foundation Models (FMs):\nAWS offers a curated collection of fully managed foundation models from top AI providers (Anthropic, OpenAI, Meta, etc.), enabling customization for diverse tasks without training models from scratch.\nPrompt Engineering Techniques:\nMastering how to steer models effectively through various prompting strategies:\nZero-shot: Model receives only the task description, no examples. Few-shot: Model is provided several examples to learn from. Chain-of-Thought: Encouraging the model to demonstrate reasoning steps for enhanced accuracy. Retrieval Augmented Generation (RAG):\nBoosts output quality by injecting external knowledge:\nR – Retrieval: Extracts relevant data from a knowledge repository. A – Augmentation: Incorporates this data as context within the prompt. G – Generation: Model produces more grounded and precise responses. Applications: Context-aware chatbots, advanced search capabilities, and real-time data summarization. Amazon Titan Embeddings:\nCompact embedding model transforming text into dense vectors for similarity search and RAG workflows, featuring multilingual support.\nAWS AI Services: Ready-to-use AI APIs:\nRekognition – Image/Video analysis Translate – Language translation Textract – Document text \u0026amp; layout extraction Transcribe – Speech-to-text conversion Polly – Text-to-speech synthesis Comprehend – Natural language processing Kendra – Enterprise intelligent search Lookout – Anomaly detection Personalize – Recommendation engines Demo Highlight:\nA facial recognition application (AMZPhoto) illustrating AI services integration into user-facing products.\n2. Amazon Bedrock AgentCore – Framework for Production-Ready AI Agents A novel framework empowering teams to operate AI agents reliably at scale:\nExecute and scale agent workflows securely Handle long-term memory management Provide fine-grained identity \u0026amp; access control Integrate with tools including Browser Tool, Code Interpreter, Memory Store Deliver observability and auditing capabilities Support popular agent frameworks (CrewAI, LangGraph, LlamaIndex, OpenAI Agents SDK, etc.) Core Insights Bedrock as the central GenAI platform: Streamlined access to multiple FMs in a unified location. Prompting + RAG as customization mechanisms: Enhance model relevance through context and examples. Embeddings for intelligent search: Titan Embeddings elevates retrieval accuracy for knowledge-driven applications. Pre-built models accelerate development timelines: Eliminates the need to construct everything manually. AgentCore minimizes deployment complexity: Manages scaling, memory, identity, and monitoring for agentic systems. Practical Applications Session concepts (particularly RAG and AgentCore) can be directly aligned with upcoming internal projects involving GenAI-powered features. Comprehensive understanding of AWS AI services facilitates selecting appropriate tools for varying application requirements, accelerating development cycles. Acquired knowledge about prompt engineering can be leveraged to enhance AI model performance across multiple scenarios. Event Experience Participating in the \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; workshop delivered an exceptionally valuable experience, offering comprehensive insights into modernization approaches for applications and databases using contemporary tools and methodologies. Memorable highlights included:\nSecured top 5 placement in the end-of-event Kahoot Quiz and photographed with speakers. Established a small collaborative team named \u0026ldquo;Mèo Cam Đeo Khăn\u0026rdquo; (Orange Cat with Scarf), combining members from \u0026ldquo;The Ballers\u0026rdquo; and \u0026ldquo;Vinhomies\u0026rdquo;. Event Photography "},{"uri":"https://rinnn465.github.io/aws-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Takeaways: \u0026ldquo;AWS Cloud Mastery Series #2 – DevOps on AWS\u0026rdquo; Program Goals Explore Infrastructure as Code (IaC) concepts alongside supporting toolchains. Guide participants through CI/CD pipeline construction and AWS DevOps services overview. Demonstrate effective monitoring and observability implementation with native AWS services. Present comprehensive overview of container workloads and deployment models on AWS. Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (TymeX) Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Key Highlights 1. Building a DevOps Foundation The speakers highlighted that DevOps is less about job titles and more about adopting habits that improve delivery:\nAutomating repetitive tasks Sharing knowledge across roles Continuously experimenting and learning Tracking measurable outcomes instead of relying on assumptions They also shared common pitfalls that early DevOps learners face, such as relying too much on tutorials without building real projects or comparing progress with others instead of focusing on steady improvement.\n2. Infrastructure as Code in Practice Rather than sticking to one tool, the speakers compared several IaC approaches:\nCloudFormation as AWS’s native template engine CDK for developers who prefer writing infrastructure using programming languages Terraform for teams working across multiple cloud providers The differences between Stacks, Constructs, State files, and abstraction layers were explained using practical examples.\nA strong message from this section: infrastructure rebuilt using IaC tends to be more consistent and maintainable than anything configured manually.\n3. Containers and Deployment Models The container portion took a step-by-step approach, starting with what a Dockerfile represents and how images are created.\nFrom there, the talk expanded into AWS services:\nECR for storing and scanning container images ECS \u0026amp; EKS for orchestration App Runner for teams who want to deploy without dealing with cluster management The comparison between ECS and EKS helped clarify when each should be used.\n4. Monitoring and Tracing The final portion of the event covered AWS CloudWatch and X-Ray:\nCloudWatch as the central source for metrics, dashboards, alarms, and logs X-Ray for visualizing request flows and identifying latency bottlenecks The speakers stressed that observability is not something added at the end—it must be integrated into the architecture from the beginning.\nLearning Outcomes Monitoring and observability require integration from initial architecture phases, not as afterthoughts, to guarantee system stability and rapid incident response. Comprehending distinctions among AWS container services (ECR, ECS, EKS, App Runner) enables optimal solution selection for specific scenarios. IaC tooling decisions (CloudFormation, CDK, Terraform) should align with team capabilities and project constraints. Infrastructure as Code (IaC) provides superior infrastructure consistency and maintainability versus manual configuration approaches. Automation, knowledge distribution, continuous experimentation, and outcome measurement form essential pillars for DevOps achievement. DevOps transcends job titles—it embodies a mindset and habit framework for enhancing software delivery. Real-World Application Scenario: AI Chatbot Development on AWS Post-workshop knowledge will be applied to an AI Chatbot project when opportunities arise. Implementation strategy includes:\nInfrastructure as Code Strategy: Utilize AWS CDK to codify complete infrastructure stack (Lambda functions, API Gateway endpoints, DynamoDB tables, S3 buckets, IAM roles, etc.), facilitating straightforward reusability and consistent scaling. CI/CD Pipeline Implementation: Deploy AWS CodePipeline to automate build, testing, and production deployment workflows, guaranteeing all code modifications undergo automated testing and deployment. Through DevOps methodologies and AWS service integration, the AI chatbot project can achieve accelerated development, continuous deployment capabilities, and maintain effortless scalability when required.\nWorkshop Impressions The workshop delivered practical insights into modern organizational approaches to DevOps implementation on AWS. Speakers blended theoretical frameworks with extensive real-world case studies, illuminating tools such as CloudFormation, CDK, Terraform, plus container operations and deployment methodologies on AWS.\nThe event provided excellent networking opportunities with peers sharing similar interests and absorbing practical wisdom from experienced practitioners.\nEvent photography "},{"uri":"https://rinnn465.github.io/aws-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Takeaways: \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop\u0026rdquo; Program Focus This workshop revolved around the security pillar within the AWS Well-Architected Framework, delivering an in-depth exploration of five critical security areas:\nArea 1: Identity \u0026amp; Access Management (IAM) Area 2: Continuous Monitoring \u0026amp; Detection Area 3: Infrastructure Protection Area 4: Data Security Area 5: Incident Response Additionally, the session featured an overview of the AWS Cloud Clubs initiative and community-driven programs that foster cloud learning in academic settings.\nSpeakers Le Vu Xuan An - AWS Cloud Club Captain HCMUTE\nTran Duc Anh - AWS Cloud Club Captain SGU\nTran Doan Cong Ly - AWS Cloud Club Captain PTIT\nDanh Hoang Hieu Nghi - AWS Cloud Club Captain HUFLIT\nHuynh Hoang Long - AWS Community Builders\nDinh Le Hoang Anh - AWS Community Builders\nNguyen Tuan Thinh - Cloud Engineer Trainee\nNguyen Do Thanh Dat - Cloud Engineer Trainee\nVan Hoang Kha - Cloud Security Engineer, AWS Community Builder\nThinh Lam - FCJ Member\nViet Nguyen - FCJ Member\nMendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect\nTinh Truong - Platform Engineer at TymeX, AWS Community Builder\nKey Highlights Launching with AWS Cloud Club The program kicked off with an introduction to the AWS Cloud Club program:\nAn environment for cultivating cloud computing capabilities through hands-on projects\nDirect access to mentorship from AWS industry experts\nBuilding networks and fostering collaboration among student communities\nOperated by clubs at HCMUTE, SGU, PTIT, and HUFLIT\nCore message: Cloud Clubs serve as a bridge for students to mature professionally, technically, and in their career relationships.\nIdentity \u0026amp; Access Management (IAM) IAM emerged as one of the workshop\u0026rsquo;s most critical topics. Primary insights included:\nImplement the principle of Least Privilege\nDelete root access keys immediately following initial configuration\nMinimize wildcard permission usage (*)\nDeploy AWS SSO for unified permission control across multiple accounts\nMaster SCPs as organizational-level permission constraints\nUtilize Permission Boundaries to restrict user and role capabilities\nThe session provided detailed analysis of two MFA approaches—TOTP and FIDO2—examining their respective security strengths and recovery mechanisms.\nThe Secrets Manager credential rotation workflow (create → set → test → finalize) was highlighted as an essential practice for secure credential management.\nContinuous Monitoring \u0026amp; Detection This module explained AWS\u0026rsquo;s approach to establishing multi-tiered observability:\nManagement Events (API invocations, configuration modifications)\nData Events (object-level operations on S3, Lambda execution records)\nNetwork Events (VPC Flow Logs)\nUnified visibility spanning all organizational accounts\nEventBridge received significant attention regarding alerting and automation—particularly its cross-account event routing capability for building centralized security workflows.\nThe \u0026ldquo;Detection-as-Code\u0026rdquo; methodology was illustrated through CloudTrail Lake and IaC-based deployment of detection logic.\nAmazon GuardDuty GuardDuty was presented as a fully managed threat intelligence service that examines CloudTrail events, VPC Flow Logs, and DNS queries to identify risks including logging deactivation, abnormal network patterns, or suspicious domain requests.\nThe workshop also explored enhanced capabilities such as malware detection for S3, EKS audit log analysis, RDS anomaly identification, Lambda network inspection, and runtime safeguards through the GuardDuty Agent.\nSpeakers demonstrated how GuardDuty aligns with established frameworks including AWS Foundational Security Best Practices and CIS Benchmarks.\nNetwork Security Management Infrastructure protection content encompassed:\nAttack classifications: Ingress, Egress, and Insider threats\nContrasting Security Groups (stateful) with NACLs (stateless)\nImplementing Route 53 Resolver for hybrid environment DNS\nAWS Network Firewall applications: egress control, network segmentation, IDS/IPS\nCombining GuardDuty threat intelligence for automatic threat blocking\nData Security \u0026amp; Governance The module emphasized the importance of encryption and credential management in AWS:\nKMS architecture (CMK → Data Key)\nLeveraging IAM conditions to restrict encryption operations\nACM for automated certificate provisioning and renewal\nCredential rotation workflows via Secrets Manager\nMandating TLS for S3 and DynamoDB access\nConfiguring SSL/TLS on RDS with server verification\nIncident Response \u0026amp; Prevention Prevention Guidelines: The session stressed prioritizing temporary credentials, preventing public S3 bucket exposure, placing sensitive services within private subnets, applying IaC to maintain consistency, and implementing dual-layer approval for high-risk modifications (code review combined with deployment pipeline).\nResponse Workflow: The workshop presented a five-phase approach: advance preparation, detection and analysis, threat containment through resource isolation or access revocation, system restoration during eradication and recovery, and post-incident knowledge documentation.\nPractical Application Security principles learned from this workshop can be directly integrated into our AI Chatbot development. Enhanced IAM practices enable tighter access management for backend systems and administrative interfaces. The emphasis on logging, monitoring, and infrastructure safeguards provides our team with a concrete framework for tracking system behavior and minimizing configuration errors. The incident response structure allows our team to address issues more promptly and methodically. Collectively, these insights contribute to building a more robust and trustworthy chatbot platform.\nEvent Impressions The program was professionally organized with expert speakers who made complex security concepts accessible. Interactive discussion segments enabled participants to resolve questions and strengthen comprehension. Some event photos "},{"uri":"https://rinnn465.github.io/aws-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Dang Truong Hung\nPhone Number: 0937726869\nEmail: fptutruonghung@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Manual booking systems or traditional chatbots often encounter difficulties when the customer volume increases, resulting in poor user experience and higher staffing costs for page management. This workshop addresses the issue by building an architecture capable of:\nAutomation: Handling booking, rescheduling, and cancellation 24/7.\nContext understanding: Accurately identifying user intents.\nData retrieval: Responding to complex questions about available time slots and consultant information.\nSolution Architecture The system is designed using an Event-Driven Serverless model combined with VPC-secured networking to ensure security and scalability:\nFrontend Interface: Users interact via Facebook Messenger.\nRequest Handling:\nAmazon API Gateway receives Webhooks from Facebook.\nAWS Lambda (WebhookReceiver) sends messages to an Amazon SQS (FIFO) queue to guarantee ordering and asynchronous processing.\nBrain (Processing Core):\nChatHandler Lambda: Manages conversations and checks sessions from Amazon DynamoDB.\nAuthentication: User verification through Email OTP using Amazon SES.\nAI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3 Haiku (for Intent Classification), Claude 3.5 Sonnet, and Claude 3 Sonnet (for Text-to-SQL \u0026amp; Extraction).\nAmazon RDS (PostgreSQL): Stores business data (Appointments, Customers, Consultants).\nText2SQL Handler: Converts natural language questions into safe SQL queries to retrieve data.\nAdmin Dashboard \u0026amp; Consultant Portal: A management interface (ReactJS) hosted on Amazon S3 and delivered through CloudFront, allowing administrators to view statistics and manage appointments. Consultant Portal provides the ability to manage booked appointments with customers and view personal schedules.\n(The image illustrates the overall architecture from the Proposal)\nKey Technologies In this workshop, you will work with the following core AWS services:\nAmazon Bedrock: The AI engine, providing Foundation Models (Claude, Titan) for language processing and SQL generation.\nAWS Lambda \u0026amp; API Gateway: Serverless backend without managing servers.\nAmazon RDS (PostgreSQL) + pgvector: Relational database with integrated vector search for RAG.\nAmazon DynamoDB: High-speed storage for session and conversation context.\nAmazon SQS: Ensures reliability and decoupling for message processing.\nAWS CDK (Cloud Development Kit): Deploys the entire infrastructure as code (IaC) using Python.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.01-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get to know members of the First Cloud Journey (FCJ) program and understand the rules and regulations at the internship unit. Understand basic concepts of cloud computing and AWS overview: AWS services, global infrastructure, AWS policies and best practices. Successfully create and configure an AWS Free Tier account. Practice basic Labs on account creation, security setup with MFA (Multi-Factor Authentication), create IAM User/Group to manage account following best practices. Understand and practice AWS cost management through AWS Budgets: Cost Budget, Usage Budget, Reservation Budget, Savings Plan Budget. Complete Module 1 of AWS Study Group to master foundational AWS Cloud knowledge. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Get to know members of the FCJ program - Read and understand the rules and regulations at the internship unit - Learn about cloud computing, basic AWS overview through Module 1 of AWS Study Group: + AWS services (Compute, Storage, Database, Networking, Security…) + AWS global infrastructure (Regions, Availability Zones) + AWS policies, best practices and how to contact AWS Support 08/09/2025 08/09/2025 AWS Study Group - Module 1 3 - Watch and complete basic Labs: + Lab01-1: Create AWS Free Tier account + Lab01-2: Set up security with Virtual MFA Device + Lab01-3: Create Admin Group and Admin User + Lab04-4: Configure Account Authentication Support - Successfully created and set up AWS Free Tier account with full security 09/09/2025 09/09/2025 AWS Study Group Labs 4 - Complete Labs on AWS cost management (AWS Budgets): + Lab07-1: Create Budget using Template + Lab07-2: Create Cost Budget (track costs) + Lab07-3: Create Usage Budget (track usage) + Lab07-4: Create Reservation Instance (RI) Budget + Lab07-5: Create Savings Plan Budget + Lab07-6: Clean Up Budgets - Understand cost management and optimization on AWS 10/09/2025 10/09/2025 AWS Budgets Labs 6 - Complete tasks following instructions to receive $100 free credit from AWS Free Tier - Learn how to maximize benefits from AWS Free Tier - Complete requirements to activate credit 12/09/2025 12/09/2025 AWS Credit Guide Week 1 Achievements: Foundational AWS knowledge\nClearly understand Cloud Computing concept and reasons why businesses move to AWS. Grasp AWS overview: main services (Compute, Storage, Database, Networking, Security), global infrastructure (Regions, Availability Zones). Understand AWS policies and best practices, how to approach AWS Support when needed. Hands-on with AWS Free Tier\nSuccessfully created AWS Free Tier account and set up initial security measures. Configured Virtual MFA Device to enhance AWS account security. Created Admin Group and Admin User following best practices, minimizing root account usage. Completed Account Authentication Support setup. AWS Cost Management\nClearly understand types of AWS Budgets: Cost Budget, Usage Budget, Reservation Budget, Savings Plan Budget. Practiced creating and managing different types of Budgets to track costs and usage on AWS. Learned how to clean up unnecessary Budgets to avoid unwanted notifications. Completed steps to receive $100 free credit from AWS Free Tier. Lab Practice Skills\nSuccessfully completed 10+ basic Labs on account creation, security, and cost management. Got familiar with AWS Management Console and basic features. Learned how to search for documentation and follow instructions to perform tasks on AWS. "},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with FCJ, cloud computing, creating AWS Free Tier account and cost management\nWeek 2: Amazon VPC, network security and VPC connection models\nWeek 3: Route 53 Resolver, VPC Peering and AWS Transit Gateway\nWeek 4: Virtual Machine, Compute \u0026amp; Storage - EC2, S3, EFS, FSx, MGN\nWeek 5: AWS IAM - Shared Responsibility Model, Users, Groups, Roles, Policies\nWeek 6: Database Concepts, AWS Database Services - RDS, Aurora, Redshift, ElastiCache\nWeek 7: AWS Architecture Design, pre-architecture development and Text-to-SQL research\nWeek 8: Comprehensive Knowledge Review \u0026amp; Complete AWS First Cloud Journey Midterm Exam\nWeek 9: Architecture Deployment \u0026amp; AWS Bedrock Chatbot Proposal\nWeek 10: Deploy Admin Dashboard on S3 \u0026amp; CloudFront, Research Amazon Cognito\nWeek 11: Built Admin Dashboard Foundation with Core UI Components, Integrated Cognito/OAuth, and Implemented Initial CRUD Management Pages.\nWeek 12: Completed Theme Switching, Optimized AWS/Database Architecture, and Enhanced UI/UX with Charting and Scheduling.\nWeek 13: Launched Full Consultant Portal with Cognito Authentication, Integrated SES Email Notifications, and Implemented Vietnamese Localization.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.2-prerequiste/","title":"Prerequisite","tags":[],"description":"","content":"MeetAssist text-to-SQL chatbot using Amazon Bedrock, Amazon DynamoDB, and Amazon RDS To manually create a virtualenv on macOS and Linux:\n$ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv.\n$ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this:\n% .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies.\n$ pip install -r requirements.txt Prerequisites The following are needed in order to proceed with this post:\nAn AWS account. A Facebook account connected to Facebook Developers for creating the Messenger chatbot. A Facebook Page that will be used to host the chatbot (create a new page or use an existing one). A Git client to clone the source code provided. Docker installed and running on the local host or laptop. Install AWS CDK The AWS Command Line Interface (AWS CLI). The AWS Systems Manager Session Manager plugin. Amazon Bedrock model access enabled for Anthropic Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Haiku and Amazon Titan Embeddings G1 – Text in the ap-northeast-1 Region. Python 3.12 or higher with the pip package manager. Node.js (version 18.x or higher) and npm – required for running the dashboard, installing dependencies, and building workshop assets. "},{"uri":"https://rinnn465.github.io/aws-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Intelligent Scheduling Chatbot on AWS A Serverless Solution for Automated Appointment Booking \u0026amp; Support 1. Executive Summary The Intelligent Scheduling Chatbot on AWS is a fully serverless solution designed to automate appointment booking and customer support through Facebook Messenger. The system integrates Large Language Models (LLMs) on Amazon Bedrock (Claude 3.5 Sonnet, Claude 3 Haiku, Claude 3 Sonnent) using a Text-to-SQL mechanism to query/execute SQL on a PostgreSQL database. It ensures accurate, context-aware responses and reduces manual scheduling workload by 80–90% while maintaining enterprise-grade security within the Asia Pacific (Tokyo) region.\n2. Problem Statement What’s the Problem? Manual appointment scheduling creates a significant workload for staff, leading to inefficiencies and delayed response times. There is a lack of automated handling for complex queries (e.g., checking specific availability or modifying bookings), and existing support channels often suffer from human error or unavailability outside business hours.\nThe Solution The platform uses Amazon Bedrock to leverage Foundation Models (Claude 3 Family) for intent detection and Text-to-SQL generation. AWS Lambda and Amazon API Gateway handle business logic and Facebook Webhook integration. Amazon RDS (PostgreSQL) stores structured appointment data, while Amazon DynamoDB manages session context. Amazon SES provides secure OTP verification. The system offers an Admin Dashboard hosted on Amazon S3 and CloudFront for consultants to manage schedules efficiently.\nBenefits and Return on Investment The solution reduces manual scheduling effort by 80–90% and ensures an end-to-end response time of ≤ 5 seconds. It provides 99.9% uptime through serverless architecture and ensures strict data security via VPC isolation and Cognito authentication. The estimated monthly cost is $37.24 USD, providing a cost-effective, pay-as-you-go model that scales with traffic, eliminating the need for expensive fixed-server infrastructure.\n3. Solution Architecture The platform employs a Serverless-First and Event-Driven architecture hosted in the ap-northeast-1 region. Incoming messages are buffered via Amazon SQS before being processed by Lambda functions that interact with Amazon Bedrock for AI logic and RDS for data persistence. The architecture is detailed below:\nAWS Services Used Amazon Bedrock: Access to Claude 3 Haiku (Intent), Claude 3 Sonnet (Context), and Claude 3.5 Sonnet (Text2SQL). AWS Lambda: Handles ChatHandler, Text2SQL logic, and Webhook processing. Amazon API Gateway: Secure entry point for Facebook Webhooks and Dashboard APIs. Amazon RDS (PostgreSQL): Stores consultants, customers, and appointment data. Amazon DynamoDB: Manages user sessions and conversation context cache. Amazon SQS (FIFO): Queues incoming messages for asynchronous processing. Amazon SES: Sends emails to users. Amazon Cognito: Manages authentication for the Admin Dashboard. Component Design Frontend Layer: Facebook Messenger via Webhook and an Admin Dashboard (React) hosted on S3/CloudFront. Processing Layer: Lambda functions execute business logic; SQS ensures message ordering; Bedrock handles AI reasoning. Data Layer: RDS for relational data (private subnet); DynamoDB for high-speed session retrieval. Security: VPC Endpoints for private communication; Secrets Manager for credentials; IAM least privilege enforcement. User Management: Amazon Cognito for staff; Facebook Messenger HMAC signature for end-users. 4. Technical Implementation Implementation Phases This project follows an Agile Scrum methodology divided into 4 key phases:\nAWS Learning \u0026amp; Lab Practice: Focus on Serverless fundamentals (Lambda, API Gateway, RDS) and IAM security (September – October). Research \u0026amp; Development: Finalize Text-to-SQL architecture, select Bedrock models, and design the SQL schema (Early November). Core System Development: Build WebhookReceiver, ChatHandler, Text-to-SQL module, and integrate Amazon Bedrock (Mid – Late November). Infrastructure \u0026amp; Dashboard: Deploy VPC/Infrastructure via CDK, develop the Admin Dashboard, and perform end-to-end testing (Late November – December). Technical Requirements\nCore Stack: Python 3.12 (Backend), TypeScript/React (Frontend), AWS CDK v2 (IaC). AI/ML Requirements: Access to Anthropic Claude 3 Haiku, Claude 3.5 Sonnet, Claude 3 Sonnent and Amazon Titan Embeddings G1 via Amazon Bedrock. Database: PostgreSQL with unaccent extension for Vietnamese language support; CSV data imports for initialization. External Dependencies: Meta (Facebook) Developer Account for Graph API v18.0; Verified Amazon SES identity. 5. Timeline \u0026amp; Milestones Project Timeline\nSeptember – October: AWS Learning \u0026amp; Hands-On Labs. Early November: Architecture Design \u0026amp; Technology Selection. Mid November: Core Backend Development (ChatHandler, SQL Module). Late November: Infrastructure Deployment \u0026amp; Admin Dashboard Frontend. December: Integration Testing, Optimization, and Final Presentation. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs AWS Services: Amazon VPC: $20.45/month (2 Interface Endpoints for security). Amazon Bedrock: $9.08/month (Tokens for Claude 3 Haiku/Sonnet and Claude 3.5 Sonnent models). Amazon RDS: $5.83/month (db.t3.micro, 20GB storage). Amazon Cognito: $0.50/month (10 MAU). Amazon SQS: $0.50/month (1M requests). AWS Secrets Manager: $0.40/month. Amazon DynamoDB: $0.28/month (On-demand). Amazon SES \u0026amp; S3: \u0026lt;$0.10/month. API Gateway, Lambda, EventBridge: $0.00 (Covered by Free Tier). Total: $37.24/month, ~$446.88/12 months.\n7. Risk Assessment Risk Matrix LLM Accuracy (Hallucinations): Medium impact, medium probability. Cost Variance: Low impact, medium probability (Pay-as-you-go fluctuations). Data Privacy: High impact, low probability. SQL Generation Failure: High impact, medium probability. Insufficient Context (Contextual Awareness): High impact, low probability. OTP Brute-force Attack: High impact, low probability. Mitigation Strategies Accuracy: Strict prompt engineering and database user permission guardrails (SELECT/INSERT only). Cost: VPC Endpoint optimization (remove when idle) and Free Tier usage. Privacy: Database in Private Isolated Subnets; PII compliance governance. SQL Generation: Prompt validation is strictly implemented to handle enum constraints, verify column/table names, and correctly format WHERE clauses (including LIKE operators) for Vietnamese names/text. Context: System prompts include mandatory function calls to retrieve additional session context, ensuring the LLM fully understands the user\u0026rsquo;s message history before responding. OTP Security: Rate limiting is enforced (max 5 attempts/session, 15s cooldown), with auto-blocking for 3600s after failures. OTPs expire in 5 minutes, and verification uses HMAC timing-attack safe methods. 8. Expected Outcomes Technical Improvements: Automated, context-aware scheduling via SQL generation. End-to-end response latency ≤ 5 seconds. Secure, VPC-isolated infrastructure with 99.9% uptime.\nLong-term Value Scalable foundation for omni-channel expansion (Web/Mobile). Data collection for future advanced analytics (AWS Glue/Athena). Operational excellence through reduced manual administrative work.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.02-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deeply understand Amazon VPC (Virtual Private Cloud): CIDR, Subnet, Route Table, ENI, Elastic IP, VPC Endpoint, Internet Gateway. Master knowledge of network security on AWS: Security Group (stateful), Network ACL (stateless), VPC Flow Logs. Understand and practice VPC connection models: VPC Peering, Transit Gateway and limitations of each method. Learn about Hybrid connectivity: VPN Site-to-Site, VPN Client-to-Site, AWS Direct Connect. Thoroughly understand types of Elastic Load Balancing (ELB): Application Load Balancer (ALB), Network Load Balancer (NLB), Classic Load Balancer (CLB), Gateway Load Balancer (GLB) and use cases for each type. Practice labs on VPC and VPN Site-to-Site. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Learn theory about Amazon VPC: + VPC in Region, CIDR IPv4/IPv6, limit 5 VPC/Region + Subnet in Availability Zone, Route Table (Default vs Custom) + Elastic Network Interface (ENI), Elastic IP (EIP) + VPC Endpoint (Interface Endpoint vs Gateway Endpoint) + Internet Gateway (IGW) - auto scale out 15/09/2025 15/09/2025 AWS Study Group - Module 2 3 - Learn about network security: + Security Group (stateful, only allow rules, apply on ENI) + Network ACL (stateless, has allow/deny rules, apply on Subnet) + VPC Flow Logs (export to CloudWatch/S3) - Learn about VPC Peering: 1:1 connection, no transitive routing, no overlapping IP - Learn about Transit Gateway: central hub, Transit Gateway Attachment at AZ-level 16/09/2025 16/09/2025 AWS Study Group - Module 2 4 - Learn about VPN: + VPN Site-to-Site (Virtual Private Gateway + Customer Gateway) + VPN Client-to-Site - Learn about AWS Direct Connect: 20-30ms latency, Hosted Connections vs Dedicated Connections - Learn details about Elastic Load Balancing: + Health check, Sticky session, Access logs + ALB (Layer 7, HTTP/HTTPS, path-based routing) + NLB (Layer 4, TCP/TLS, static IP, high performance) + CLB (Layer 4+7, high cost, few features) + GLB (for Security appliances) 17/09/2025 17/09/2025 AWS Study Group - Module 2 5 - Practice Lab03 - Amazon VPC and VPN Site-to-Site (18 sub-labs): + Lab 03-1: Introduction, Subnets, Route Table, IGW, NAT Gateway + Lab 03-2: Security Group, Network ACLs, VPC Resource Map + Lab 03-3: Create VPC, Subnet, IGW, Route Table, Security Groups + Lab 03-4: Create EC2 Instances, Test connection, NAT Gateway, EC2 Instance Connect Endpoint 18/09/2025 18/09/2025 Lab03 - VPC Week 2 Achievements: Knowledge about Amazon VPC\nUnderstand VPC (CIDR IPv4/IPv6, limit 5 VPC/Region), Subnet (by AZ), Route Table (Default vs Custom). Understand ENI (virtual network card), EIP (static public IPv4, charged when not in use). Differentiate VPC Endpoint: Interface Endpoint (ENI + Private IP) and Gateway Endpoint (Route Table, only S3/DynamoDB). Understand Internet Gateway (IGW) - auto scale out. Knowledge about network security\nDifferentiate Security Group (stateful, only allow, apply on ENI) and Network ACL (stateless, allow/deny, apply on Subnet). Understand VPC Flow Logs (export to CloudWatch/S3, does not capture packet content). Knowledge about VPC and Hybrid connectivity\nVPC Peering (1:1 connection, no transitive routing, no overlapping IP). Transit Gateway (central hub, Transit Gateway Attachment at AZ-level). VPN Site-to-Site (Virtual Private Gateway + Customer Gateway), VPN Client-to-Site. AWS Direct Connect (20-30ms latency, Hosted/Dedicated Connections). Knowledge about Elastic Load Balancing\nUnderstand 4 types of ELB: ALB (Layer 7, HTTP/HTTPS, path-based routing), NLB (Layer 4, TCP/TLS, static IP, high performance), CLB (Layer 4+7, rarely used), GLB (Security appliances). Understand health check, sticky session, access logs. Hands-on skills\nCompleted Lab03 (18 sub-labs): create VPC, Subnet, IGW, NAT Gateway, Security Group, NACL, EC2 Instances, Route Table, test connection. Mastered the process of setting up complete VPC and configuring connections between subnets. ✅ After week 2, have solid foundation on Networking in AWS (VPC, Security, Routing), connection models (Peering, Transit Gateway, VPN, Direct Connect), Load Balancing (ALB, NLB, CLB, GLB) and practiced 18 labs on VPC.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Data Protection and Security Best Practices with Veeam on AWS This blog explores comprehensive data protection and security strategies for AWS environments using Veeam. You will learn best practices for implementing multi-layered backup strategies, encryption everywhere (in transit and at rest), IAM management, and repository immutability to protect against ransomware. The article covers how Veeam integrates with AWS services like S3 Object Lock, AWS KMS, and private networking to ensure data security, compliance with regulations (HIPAA, NIST, ISO 20000), and robust disaster recovery capabilities.\nBlog 2 - Effectively implementing resource control policies in a multi-account environment This blog provides a comprehensive guide to implementing Resource Control Policies (RCPs) in AWS multi-account environments. You will learn how to establish data perimeters, prevent cross-service confused deputy risks, and enforce consistent access controls across AWS resources. The article walks through a five-phase implementation journey: identifying control objectives, designing permissions guardrails with proper exception handling, anticipating impacts using IAM Access Analyzer and CloudTrail, progressive deployment strategies, and ongoing monitoring to ensure your security controls align with business objectives.\nBlog 3 - Dynamic text-to-SQL for enterprise workloads with Amazon Bedrock Agents This blog demonstrates how to build intelligent, context-aware text-to-SQL solutions using Amazon Bedrock Agents for enterprise applications. You will learn how to create agentic workflows that dynamically generate SQL queries from natural language, handle complex multi-step reasoning, and adapt to changing database schemas. The article covers implementing retrieval-augmented generation (RAG) for schema discovery, using action groups for query execution, managing conversation context, and ensuring accuracy through guardrails and validation—enabling non-technical users to query enterprise databases naturally.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.3-enablebedrockmodels/","title":"Enable Bedrock Models","tags":[],"description":"","content":"Enabling Bedrock Models Before deploying the solution, you need to enable the required Amazon Bedrock models in your AWS account.\nSteps to Enable Models Search for Amazon Bedrock in AWS Console 2. Access Model catalog from the left navigation menu\n3. Choose the corresponding model name:\nAnthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Amazon Titan Embeddings G1 – Text 4. Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to enable each model\nMake sure you enable all four models in the ap-northeast-1 (Tokyo) region as the solution is deployed in this region.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.03-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Practice labs on Route 53 Resolver to set up Hybrid DNS between AWS and on-premises. Practice VPC Peering to connect 2 VPCs, configure Security Groups, NACLs, Route Tables and Cross-Peer DNS. Practice AWS Transit Gateway to connect multiple VPCs through a central hub, configure Transit Gateway Attachments and Route Tables. Strengthen Networking knowledge on AWS through hands-on labs. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Practice Lab10 - Hybrid DNS with Route 53 Resolver (11 sub-labs): + Lab10-01: Introduction - Learn about Hybrid DNS + Lab10-02.1: Generate Key Pair + Lab10-02.2: Initialize CloudFormation Template + Lab10-02.3: Configure Security Group + Lab10-03: Connect to RDGW (Remote Desktop Gateway) + Lab10-05: Set up DNS + Lab10-05.1: Create Route 53 Outbound Endpoint + Lab10-05.2: Create Route 53 Resolver Rules + Lab10-05.3: Create Route 53 Inbound Endpoints + Lab10-05.4: Test results + Lab10-06: Clean up resources 22/09/2025 22/09/2025 Lab10 - Route 53 Resolver (https://000010.awsstudygroup.com/) 3 - Practice Lab19 - VPC Peering (8 sub-labs): + Lab19-1: Introduction - Set up VPC Peering + Lab19-2.1: Initialize CloudFormation Templates + Lab19-2.2: Create Security Group + Lab19-2.3: Create EC2 Instance + Lab19-3: Update Network ACL (NACLs) + Lab19-4: Create a Peering Connection + Lab19-5: Configure Route Tables + Lab19-6: Enable Cross-Peer DNS 23/09/2025 23/09/2025 Lab19 - VPC Peering (https://000019.awsstudygroup.com/) 4 - Practice Lab20 - AWS Transit Gateway (7 sub-labs): + Lab20-1: Introduction - Set up AWS Transit Gateway + Lab20-2: Preparation steps + Lab20-3: Create Transit Gateway + Lab20-4: Create Transit Gateway Attachments + Lab20-5: Create Transit Gateway Route Tables + Lab20-6: Add Transit Gateway Routes to VPC Route Tables + Lab20-7: Clean up resources 25/09/2025 25/09/2025 Lab20 - Transit Gateway (https://000020.awsstudygroup.com/) Week 3 Achievements: Lab10 - Hybrid DNS with Route 53 Resolver (11 sub-labs)\nSet up Hybrid DNS to connect DNS between AWS VPC and on-premises data center. Create and configure Route 53 Outbound Endpoint to resolve domain names from VPC to on-premises. Create and configure Route 53 Inbound Endpoint to resolve domain names from on-premises to VPC. Create Route 53 Resolver Rules to route DNS queries. Use CloudFormation to initialize infrastructure, configure Security Groups for DNS traffic. Connect to Remote Desktop Gateway (RDGW) to manage Windows environment. Test results and verify DNS resolution works correctly between the 2 environments. Lab19 - VPC Peering (8 sub-labs)\nSet up VPC Peering Connection to connect 2 VPCs together. Use CloudFormation Templates to initialize VPC infrastructure. Create and configure Security Groups to allow traffic between 2 VPCs. Launch EC2 Instances in VPCs to test connectivity. Update Network ACL (NACL) to allow traffic between subnets. Configure Route Tables in both VPCs to route traffic through peering connection. Enable Cross-Peer DNS to resolve DNS names between VPCs. Test and verify connectivity between EC2 instances in 2 VPCs. Lab20 - AWS Transit Gateway (7 sub-labs)\nSet up AWS Transit Gateway as central hub to connect multiple VPCs. Perform preparation steps and planning for Transit Gateway architecture. Create Transit Gateway with appropriate configuration for multi-VPC environment. Create Transit Gateway Attachments to attach VPCs to Transit Gateway. Create and configure Transit Gateway Route Tables to control traffic routing. Update VPC Route Tables to route traffic through Transit Gateway. Test connectivity between VPCs through Transit Gateway. Clean up resources after completing lab. ✅ After week 3, completed 26 labs on Hybrid DNS, VPC Peering and Transit Gateway, strengthened networking skills on AWS and understand how to connect VPCs together and with on-premises.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3 – Security Pillar Workshop\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.4-configureawscli/","title":"Configure AWS CLI","tags":[],"description":"","content":"Configuration AWS CLI To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your credentials.\nSteps Type aws configure in your terminal. Make sure you have created CLI secret access key for your account (recommend using an IAM account with admin privileges)\nComplete the configuration form:\nAWS Access Key ID: Your access key AWS Secret Access Key: Your secret key Default region name: ap-northeast-1 Default output format: json Creating IAM Access Keys To create IAM access keys, follow these steps:\nGo to AWS Console → IAM → Users → Your User Navigate to Security Credentials tab Click Create Access Key Download or save your credentials securely Never share your AWS access keys or commit them to version control. Store them securely and rotate them regularly.\nVerification After configuration, verify your setup by running:\naws sts get-caller-identity You should see your account ID, user ARN, and user ID in the response.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.04-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deeply understand Virtual Machine, Compute \u0026amp; Storage Management in AWS system:\nMaster how services like EC2, Amazon Lightsail, AWS EFS/FSX, AWS Application Migration Services (MGN), Amazon Simple Storage (S3) work. Operating Virtual Machine \u0026amp; Storage: Proficient in deploying and managing flexible computing resources (EC2) and simplified solutions (Amazon Lightsail).\nDeploy file and object storage: Effectively use managed file storage services (AWS EFS/FSX) and durable, highly scalable object storage service (Amazon Simple Storage (S3)).\nMigrate servers to Cloud: Master the process and tools for migrating applications/servers from On-premises environment to AWS using AWS Application Migration Services (MGN).\nExpand knowledge: Develop skills in reading and accurately translating AWS technical documentation to synthesize in-depth knowledge and support high-quality information sharing within the team.\nTasks to carry out this week: Day Tasks Start date Completion date References 2 - Research Compute \u0026amp; Storage services in AWS: + Amazon EC2 (Elastic Compute Cloud): Learn instance types, configure security groups, and manage EC2 lifecycle.\n• Research Instance Types (General Purpose, Compute Optimized, Memory Optimized) and Pricing Models (On-Demand, Reserved, Spot).\n+ Amazon Lightsail: Deploy simplified virtual private server, compare with EC2 regarding use cases and pricing.\n+ Amazon S3 (Simple Storage Service): Manage buckets, object storage, versioning, lifecycle policies and storage classes (Standard, IA, Glacier).\n+ AWS EFS/FSX: Learn managed file storage, differentiate between EFS (Linux) and FSx (Windows/Lustre), configure mount points. 29/09/2025 29/09/2025 3 - Deploy \u0026amp; manage EC2 Instance with best practices: + Configure User Data to automatically initialize instance on launch. + Set up Elastic IP and Network Interface for EC2. + Use EC2 Instance Connect and Session Manager for secure access. - Optimize storage with Amazon S3: + Configure S3 Bucket Policies and Access Control Lists (ACL). + Set up S3 Lifecycle Rules to automatically transition storage classes. + Practice S3: initialize S3 Bucket, Set up Notification, Create Back up plan, Test Restore. - Compare EC2 and Lightsail: + Analyze pros and cons between the two services. + Identify suitable use cases for each service (startup vs enterprise). + Evaluate costs and management complexity. 30/09/2025 30/09/2025 5 - Learn AWS EFS and FSx: + Research architecture and how Elastic File System (EFS) works. + Learn performance modes (General Purpose, Max I/O) and throughput modes. + Compare FSx for Windows File Server and FSx for Lustre. - Explore AWS Application Migration Service (MGN): + Learn migration process from on-premises to AWS Cloud. + Differentiate between MGN and other migration methods (SMS, CloudEndure). 02/10/2025 02/10/2025 6 - Translate blogs \u0026amp; documents: + Translate content from document 1: Data Protection and Security Best Practices with Veeam on AWS + Translate content from document 2: Dynamic text-to-SQL for enterprise workloads with Amazon Bedrock Agents + Translate content from document 3: Effectively implementing resource control policies in a multi-account environment 03/10/2025 03/10/2025 Google Doc 1, Google Doc 2, Google Doc 3 Week 4 Achievements: Completed learning and practicing Compute \u0026amp; Storage services in AWS, including:\nAmazon EC2 – Mastered instance types, pricing models, and virtual machine lifecycle management. Amazon Lightsail – Deploy and manage simplified VPS for small applications. Amazon S3 – Manage object storage with versioning, lifecycle policies and storage classes. AWS EFS/FSx – Deploy and configure managed file storage for Linux (EFS) and Windows/HPC (FSx). AWS Application Migration Service (MGN) – Understand server migration process from on-premises to AWS Cloud. Completed Compute \u0026amp; Storage hands-on labs:\nInitialize and manage EC2 Instance EC2 Auto Scaling \u0026amp; Load Balancing Amazon Lightsail Setup S3 Bucket Management \u0026amp; Lifecycle Policies EFS File System Configuration S3 Versioning \u0026amp; Cross-Region Replication Mastered important concepts:\nEC2 Instance Types and Pricing Models (On-Demand, Reserved, Spot). S3 Storage Classes and how to optimize storage costs. Performance modes and Throughput modes of EFS. Migration process with AWS MGN. Know how to compare and choose suitable services:\nDifferentiate between EC2 and Lightsail based on use case and complexity. Understand when to use EFS (Linux workloads) vs FSx (Windows/HPC workloads). Proficient in best practices for EC2 and S3:\nConfigure User Data, Elastic IP, Security Groups for EC2. Set up S3 Bucket Policies, ACL, and Lifecycle Rules. Use EC2 Instance Connect and Session Manager for secure access. "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.5-preparedata/","title":"Prepare Data","tags":[],"description":"","content":"Download the Dataset and Upload to Amazon S3 Step 1: Download the Dataset Navigate to the MeetAssistData Download the data to your local machine Unzip the file, which will expand into a folder called DATA IMPORTANT - Do NOT open CSV files with Excel:\nMicrosoft Excel can automatically convert data formats, which may corrupt the data (e.g., phone numbers, dates, IDs may be altered) Recommended approach: Upload the data to S3 first (steps below), then use VS Code to view/validate the CSV files instead of Excel If you must view locally: Use VS Code only - it won\u0026rsquo;t modify your data like Excel does Keep original CSV files intact before uploading to S3 Step 2: Create S3 Bucket Run this CLI script to create the Amazon S3 bucket. Replace \u0026lt;account-id\u0026gt; with your AWS account ID:\naws s3 mb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --region ap-northeast-1 To find your AWS Account ID, run: aws sts get-caller-identity --query Account --output text\nStep 3: Upload Data to S3 Go to the AWS S3 Console Navigate to the bucket you just created: meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 Create a folder named data Upload all CSV files from the unzipped DATA folder into the data folder in S3 Verification After uploading, verify that all CSV files are present in your S3 bucket under the data/ prefix:\naws s3 ls s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1/data/ You should see all the CSV files listed in the output.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.05-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Deeply understand Shared Responsibility Model in AWS and differentiate security responsibilities between AWS and customers. Master core concepts of AWS Identity and Access Management (IAM): Root Account, IAM User, IAM Group, IAM Role IAM Policy (Identity-based and Resource-based) IAM Principal and authentication/authorization mechanisms Thoroughly understand Amazon Cognito and how to manage user authentication for web/mobile apps: User Pool - Manage user registration/login Identity Pool - Grant access to AWS services Learn AWS Organizations to centrally manage multiple AWS accounts: Organization Unit (OU) Service Control Policies (SCP) Consolidated Billing Get familiar with AWS Identity Center (SSO) to centrally manage access to AWS accounts and external applications. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Learn Shared Responsibility Model: + Differentiate security responsibilities between AWS and Customer + Understand differences between Infrastructure Services, Container Services and Managed Services + Determine scope of responsibility for each AWS service type - Learn basic AWS IAM: + Root Account and best practices for protecting Root Account + IAM User, IAM Group and credentials management + IAM Principal and authentication/authorization mechanisms 06/10/2025 06/10/2025 3 - Research IAM Policy: + Identity-based Policy vs Resource-based Policy + Permission evaluation mechanism: Explicit Deny \u0026gt; Explicit Allow + Practice writing IAM Policy for S3 bucket - Learn IAM Role: + Difference between IAM User and IAM Role + Trust Policy and Permission Policy + Assume Role and AWS STS (Security Token Service) + Use cases: IAM Role for EC2, Lambda, Cross-account access 07/10/2025 07/10/2025 AWS IAM Documentation 4 - Learn Amazon Cognito: + User Pool: user registration/login, MFA, custom attributes + Identity Pool: grant access to AWS services for end users + Integrate User Pool with Identity Pool + Federated Identity with Social Login (Google, Facebook) - Practice lab: + Create User Pool and test registration/login + Configure Identity Pool to access S3 08/10/2025 08/10/2025 Amazon Cognito Documentation 5 - Research AWS Organizations: + Organization structure: Root, OU (Organization Unit), Member Accounts + Service Control Policies (SCP): Allow-list vs Deny-list + Consolidated Billing and cost allocation + Best practices for multi-account strategy - Learn AWS Identity Center (SSO): + Identity Source: AWS Identity Center, Active Directory, External IdP + Permission Sets and Account assignments + Integration with AWS Managed Microsoft AD 09/10/2025 09/10/2025 AWS Organizations AWS Identity Center 6 - Practice IAM Best Practices: + Create IAM Administrator User instead of using Root + Lock Root User credentials + Configure MFA for important accounts + Use IAM Role for applications instead of hard-coding credentials - Consolidate knowledge: + Compare IAM User, IAM Group, IAM Role + Compare User Pool vs Identity Pool in Cognito + Understand relationship between Organizations, SCP and IAM Policy 10/10/2025 10/10/2025 AWS Security Best Practices Week 5 Achievements: Clearly understand Shared Responsibility Model in AWS:\nDifferentiate security responsibilities between AWS (Security OF the Cloud) and Customer (Security IN the Cloud) Understand differences in security responsibilities between Infrastructure Services, Container Services and Managed Services Determine scope of responsibility for specific AWS service types Master AWS IAM (Identity and Access Management):\nRoot Account: Understand importance and best practices for protecting Root Account IAM User: Create and manage users, credentials (access key/secret key) IAM Group: Group users for more efficient permission management IAM Policy: Differentiate Identity-based Policy and Resource-based Policy IAM Role: Understand difference with IAM User, how to use Trust Policy and Assume Role Permission evaluation mechanism: Explicit Deny always takes priority over Allow Proficient with IAM Role and important use cases:\nGrant EC2 instance access to S3 without hard-coding credentials Use AWS STS (Security Token Service) to create temporary credentials Cross-account access with IAM Role IAM Role for AWS Services (Lambda, EC2, ECS, etc.) Thoroughly understand Amazon Cognito:\nUser Pool: Manage authentication (registration/login), MFA, custom attributes Identity Pool: Grant authorization access to AWS services for end users Integrate User Pool with Identity Pool for complete flow Federated Identity with Social Login (Google, Facebook, SAML) Practice creating User Pool and configuring Identity Pool to access S3 Master AWS Organizations:\nOrganization structure: Root, Organization Unit (OU), Member Accounts Service Control Policies (SCP): Set maximum permission limits for accounts/OUs Differentiate Allow-list and Deny-list strategies Consolidated Billing: Centralize billing and optimize costs Multi-account strategy and best practices Understand AWS Identity Center (SSO):\nCentrally manage access to multiple AWS accounts Identity Source: AWS Identity Center, AWS Managed Microsoft AD, External IdP Permission Sets: Define access permissions for users/groups Integration with Active Directory (Two-way trust, AD Connector) Apply IAM Best Practices:\n✅ Don\u0026rsquo;t use Root Account for daily work ✅ Create IAM Administrator User to replace Root ✅ Lock Root User access keys ✅ Enable MFA (Multi-Factor Authentication) for important accounts ✅ Use IAM Role for applications instead of hard-coding credentials ✅ Apply Principle of Least Privilege Clearly understand relationships between components:\nIAM User vs IAM Role: When to use which User Pool vs Identity Pool in Cognito Organizations SCP vs IAM Policy: Interaction and priority Identity-based Policy vs Resource-based Policy "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-Powered Schedule Booking Chatbot on AWS Overview In this workshop, we will build MeetAssist — an intelligent chatbot system running on the AWS Serverless platform. This solution applies Generative AI (LLM) to automate the process of appointment scheduling and customer information lookup through a Facebook Messenger interface.\nInstead of responding based on a fixed rule-based script, the system uses a Text-to-SQL model to understand natural language, query real-time data from the database, and provide flexible responses to users.\nContent Workshop overview Prerequisites Enable Bedrock Models Configure AWS CLI Prepare Data Deploy Solution Setup Messenger Bot Setup Admin Dashboard Using the Chatbot Using the Admin Dashboard Using the Consultant Portal Clean Up Resources "},{"uri":"https://rinnn465.github.io/aws-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS from 08/09/2025 to 12/11/2025, I had a valuable opportunity to learn, practice, and extensively apply the knowledge acquired in school to a real-world, professional, and challenging working environment.\nI participated in the MeetAssist Project - Automated Scheduling Chatbot, a key project aimed at optimizing customer interaction and scheduling processes. Through this, I comprehensively developed many important skill groups, thereby improving my Programming, Knowledge and Usage of AWS Services, Requirements Analysis and System Design, Teamwork and Communication, Technical Report Writing and Documentation\u0026hellip;.\nIn terms of work ethic, I always maintained a high sense of responsibility, strived to complete assigned tasks well, ensured strict compliance with regulations and the team\u0026rsquo;s product development processes, and actively engaged in discussions and learning from colleagues and mentors to continuously improve work efficiency and quality.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Enhance the ability to independently search for information, grasp tasks, and propose improvement initiatives without waiting for instructions. Improve the way of conveying ideas, practice active listening, and strengthen effective coordination in team working environments. Develop the ability to analyze problems from the root, propose optimal solutions, and handle situations flexibly and professionally. "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.6-deploysolution/","title":"Deploy the Solution","tags":[],"description":"","content":"Deploy the Solution In this section, we will clone the MeetAssist repository and deploy the complete infrastructure using AWS CDK.\nStep 1: Clone the Repository Clone the repository from GitHub:\ngit clone https://github.com/AWS-Vinhomes-Chatbot/MeetAssist cd MeetAssist Step 2: Build the Dashboard Before deploying the CDK application, we need to build the frontend dashboard.\nNavigate to frontend folder cd frontend Install dependencies Run the following command to install all necessary libraries:\nnpm i Build the Dashboard After the installation is complete, run the build command:\nnpm run build Once the process completes, a dist directory will be created, containing the index.html file and the assets folder.\nReturn to project root Use this command to return to the project\u0026rsquo;s root folder:\ncd .. Step 3: Deploy the CDK Application Deploy the CDK application. It will take about 20-30 minutes to deploy all of the resources.\ncdk bootstrap aws://{{account_id}}/ap-northeast-1 cdk deploy --all If you receive an error at this step, please ensure Docker is running on the local host or laptop.\nReplace {{account_id}} with your actual AWS Account ID. You can find it by running:\naws sts get-caller-identity --query Account --output text Step 4: Initialize the Database with Embeddings After the CDK deployment completes, you must run the DataIndexer Lambda function to populate the embeddings table with database schema information.\nUse the following AWS CLI command:\naws lambda invoke \\ --function-name DataIndexerStack-DataIndexerFunction \\ --invocation-type Event \\ response.json \\ --region ap-northeast-1 The DataIndexer function generates vector embeddings of your database schema, which enables the chatbot to understand and query your data using natural language.\nStep 5: Verify Deployment After completing all the steps above, your environment is fully deployed and initialized.\nYou can verify the deployment by checking:\nCloudFormation Stacks: All stacks should show CREATE_COMPLETE status outputs.json file: Contains important URLs and IDs for your deployment S3 Buckets: Dashboard assets should be uploaded Lambda Functions: All functions should be created and ready Next Steps Now you can proceed to:\nConfigure the Facebook Messenger integration (Section 5.7) Set up the Admin Dashboard (Section 5.8) Start using the MeetAssist chatbot (Section 5.9) "},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.06-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Clearly understand foundational Database Concepts including basic concepts such as Database, Session, Primary Key, Foreign Key, Index, Partition, Execution Plan, Database Log, Buffer. Clearly differentiate between RDBMS and NoSQL, as well as between OLTP and OLAP. Master main AWS Database services: Amazon RDS - Managed relational database service Amazon Aurora - Optimized RDBMS with high performance and improved storage layer Amazon Redshift - Data Warehouse for OLAP workload with MPP architecture Amazon ElastiCache - Caching service with Redis/Memcached Practice deploying and managing Amazon RDS through detailed labs from VPC setup to backup/restore. Practice Database Migration with AWS Database Migration Service (DMS) and Schema Conversion Tool (SCT). Research RAG Chatbot and AI Agents architectures on AWS to prepare for real projects. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Learn Database Concepts: + Database, Session, Primary Key, Foreign Key + Index, Partition, Execution Plan, Database Log, Buffer - Differentiate RDBMS vs NoSQL: + Fixed vs dynamic schema + SQL vs collection-based query + Vertical scaling vs Horizontal scaling + Normalization vs Denormalization + Transaction Model: ACID vs BASE - Clearly understand OLTP vs OLAP: + OLTP: Fast transaction processing, frequent updates, focus on data integrity + OLAP: Historical data analysis, complex queries, focus on query performance 13/10/2025 13/10/2025 AWS Study Group - Database Concepts 3 - Research Amazon RDS: + Supported engines: Aurora, MySQL, PostgreSQL, MSSQL, Oracle, MariaDB + Auto backup (both log and database – max 35 days) + Read Replica to offload read workload (reporting) + Multi-AZ (Primary/Standby) with auto failover + Data encryption at rest and in transit + Storage Auto Scaling + Security Group and NACL protection - Learn Amazon Aurora: + Optimized storage layer, higher performance than regular RDS + Supports MySQL and PostgreSQL compatible + Back Track: restore DB to previous point in time + Clone: fast copy creation + Global Database: 1 Master + Multi Read in different Regions + Multi Master: Multi Master database 14/10/2025 14/10/2025 Amazon RDS Documentation Amazon Aurora Documentation 4 - Research Amazon Redshift: + Data Warehouse managed by AWS, used for OLAP workload + MPP (Massively Parallel Processing) Database architecture + Leader Node: coordinate and optimize queries + Compute Node: store and process data (compute + storage) + Columnar Storage: optimized for analytics and aggregation + PostgreSQL core but optimized for OLAP + Redshift Spectrum: query data directly from S3 + Transient Cluster: cost optimization - Learn Amazon ElastiCache: + Manage caching engines: Redis and Memcached + Auto detect and replace failed nodes + Place before DB layer to cache data, reduce DB load + Prioritize Redis for new workloads (diverse features, good performance) + Require writing and managing caching logic on application 14/10/2025 14/10/2025 Amazon Redshift Documentation Amazon ElastiCache Documentation 5 - Practice Lab 5 - Amazon RDS (detailed step by step): + Lab05-2.1: Create VPC + Lab05-2.2: Create EC2 Security Groups + Lab05-2.3: Create RDS Security Groups + Lab05-2.4: Create DB Subnet Group + Lab05-3: Create EC2 Instance + Lab05-4: Create RDS Database Instance + Lab05-5: Application Deployment + Lab05-6: Backup and Restore + Lab05-7: Cleanup Resources ✅ 15/10/2025 15/10/2025 AWS Hands-on Labs - RDS 6-CN - Practice Lab 43 - Database Migration (DMS \u0026amp; SCT) (17 sub-labs): + Lab43-01: EC2 Connect RDP Client + Lab43-02: EC2 Connect Fleet Manager + Lab43-03: SQL Server Source Config + Lab43-04: Oracle Connect Source DB + Lab43-05: Oracle Config Source DB + Lab43-06: Drop Constraint + Lab43-07: MSSQL to Aurora MySQL Target Config + Lab43-08: MSSQL to Aurora MySQL Create Project + Lab43-09: MSSQL to Aurora MySQL Schema Conversion + Lab43-10: Oracle to MySQL Schema Conversion + Lab43-11: Create Migration Task \u0026amp; Endpoint + Lab43-12: Inspect S3 + Lab43-13: Create Serverless Migration + Lab43-14: Create Event Notification + Lab43-15: Logs + Lab43-16: Troubleshoot Test Scenario - Memory Pressure + Lab43-17: Troubleshoot Test Scenario - Table Error - Read blogs and research architecture: + RAG Chatbot on AWS + Advanced Multimodal Chatbot with Speech-to-Speech - Team meeting to brainstorm pre-architecture for project 17-18/10/2025 17-18/10/2025 RAG Chatbot on AWS Multimodal Chatbot with Speech-to-Speech Week 6 Achievements: Clearly understand basic Database Concepts:\nConcepts: Session, Primary/Foreign Key, Index, Partition, Execution Plan, Buffer, Database Log Compare RDBMS vs NoSQL: Schema (fixed vs dynamic), Scaling (vertical vs horizontal), Transaction (ACID vs BASE) Differentiate OLTP vs OLAP: Transaction processing vs Analytical processing Master AWS Database Services:\nAmazon RDS: 6 engines, auto backup (35 days), Read Replica, Multi-AZ failover, encryption, auto scaling Amazon Aurora: Back Track, Clone, Global Database, Multi Master - superior performance Amazon Redshift: MPP architecture, Columnar Storage, Redshift Spectrum - optimized for OLAP Amazon ElastiCache: Redis/Memcached, auto node replacement, reduce database load Completed hands-on Labs:\n✅ Lab 5 - Amazon RDS: End-to-end deployment from VPC, Security Groups, RDS instance, application deployment to backup/restore (7 sub-labs) ✅ Lab 43 - Database Migration: DMS \u0026amp; SCT, migrate MSSQL/Oracle to Aurora MySQL, troubleshooting (17 sub-labs) Researched RAG Chatbot and AI Agents architecture on AWS, team meeting to brainstorm pre-architecture for project.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.7-setupmessengerbot/","title":"Setup Messenger Bot","tags":[],"description":"","content":"Setting Up the Messenger Chatbot The MeetAssist chatbot is integrated with Facebook Messenger, allowing users to interact naturally to book, update, or cancel appointments.\nStep 1: Create a Facebook App Go to Facebook Developers Click \u0026ldquo;My Apps\u0026rdquo; Select \u0026ldquo;Create an app\u0026rdquo; Fill in your app name (e.g., demo) and App Contact Email → Click \u0026ldquo;Next\u0026rdquo; Select \u0026ldquo;Messaging businesses\u0026rdquo; as your use case → Click \u0026ldquo;Receive\u0026rdquo; Select your \u0026ldquo;Business\u0026rdquo; profile or choose none if you just want to test the app → Click \u0026ldquo;Receive\u0026rdquo; Click \u0026ldquo;Go to Dashboard\u0026rdquo; Step 2: Configure App Settings In your app dashboard, click \u0026ldquo;App Settings\u0026rdquo; → \u0026ldquo;Basic info\u0026rdquo; Copy and save your App ID and App Secret Paste the Privacy Policy URL: https://www.freeprivacypolicy.com/live/e7193dae-4bba-4482-876e-7b76d83a0676 Select \u0026ldquo;Messenger for Business\u0026rdquo; as the app category → Click \u0026ldquo;Save Changes\u0026rdquo; Step 3: Store Facebook Credentials in AWS Run the following AWS CLI commands to securely store your Facebook credentials:\nCreate SSM Parameters: # Facebook App ID aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_id\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_ID\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App ID for MeetAssist\u0026#34; ` --region ap-northeast-1 # Facebook App Secret aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_secret\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_SECRET\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App Secret for MeetAssist\u0026#34; ` --region ap-northeast-1 Create Secrets Manager Secrets: # Facebook Page Access Token (get this from step 4 below) aws secretsmanager create-secret ` --name \u0026#34;meetassist/facebook/page_token\u0026#34; ` --description \u0026#34;Facebook Page Access Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_FACEBOOK_PAGE_ACCESS_TOKEN\u0026#34; ` --region ap-northeast-1 # Facebook Verify Token (create a random string, e.g., \u0026#34;my_secure_token_12345\u0026#34;) aws secretsmanager create-secret ` --name \u0026#34;/meetassist/facebook/verify_token\u0026#34; ` --description \u0026#34;Facebook Webhook Verify Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_CUSTOM_VERIFY_TOKEN_123456\u0026#34; ` --region ap-northeast-1 Replace YOUR_FACEBOOK_APP_ID, YOUR_FACEBOOK_APP_SECRET, YOUR_FACEBOOK_PAGE_ACCESS_TOKEN, and YOUR_CUSTOM_VERIFY_TOKEN_123456 with your actual values.\nStep 4: Connect Facebook Page and Get Page Access Token In your app dashboard, click \u0026ldquo;Use Cases\u0026rdquo; → \u0026ldquo;Customize\u0026rdquo; Go to \u0026ldquo;Install the Messenger API\u0026rdquo;, click \u0026ldquo;Connect\u0026rdquo; to link your Facebook Page to the app Select \u0026ldquo;Create\u0026rdquo; to copy the generated Page Access Token Use this token in the aws secretsmanager create-secret command above Step 5: Configure Webhooks API Get your Webhook URL from the outputs.json file (generated after CDK deployment) In your app dashboard, go to \u0026ldquo;Messenger API Settings\u0026rdquo; Go to the \u0026ldquo;Configure Webhooks\u0026rdquo; section Enter the following information: Callback URL: https://\u0026lt;your-api-gateway-url\u0026gt;/webhook (from outputs.json) Verify Token: The same random string you used in step 3 (e.g., YOUR_CUSTOM_VERIFY_TOKEN_123456) Click \u0026ldquo;Verify and Save\u0026rdquo; Subscribe to the following webhook fields: messages messaging_postbacks messaging_account_linking Step 6: Subscribe Page to Your App (This step is automatically handled when you connect the page in Step 4, so you can skip this if already done)\nStep 7: Configure Messenger Profile (Get Started Button \u0026amp; Greeting) After deployment, the chatbot automatically configures:\nGet Started Button: Users click this to begin conversation Greeting Text: Welcome message shown before user starts chatting To manually update if needed:\n# Set Get Started Button curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;get_started\u0026#34;: {\u0026#34;payload\u0026#34;: \u0026#34;GET_STARTED\u0026#34;} }\u0026#39; # Set Greeting Text curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;greeting\u0026#34;: [ { \u0026#34;locale\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Xin chào! Mình là MeetAssist, trợ lý đặt lịch hẹn tư vấn hướng nghiệp. Hãy nhấn \\\u0026#34;Bắt đầu\\\u0026#34; để sử dụng dịch vụ! 👋\u0026#34; } ] }\u0026#39; Step 8: App Mode and Permissions Development Mode: Your app is currently in development mode. Only you (the app developer) and testers you add can interact with the bot.\nPublic Access: To allow other users to use your bot, go to your app dashboard and switch the app to \u0026ldquo;Live Mode\u0026rdquo; by selecting \u0026ldquo;Post\u0026rdquo;\nApp Review: For full features and permissions, you must complete Facebook\u0026rsquo;s App Review process. For testing purposes only, App Review is not required.\nVerification Test your bot by:\nOpen your Facebook Page in Messenger Click the \u0026ldquo;Get Started\u0026rdquo; button Verify you receive the greeting message Try sending a test message to confirm the webhook is working If everything is configured correctly, the bot should respond to your messages!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe First Cloud Journey program delivered an outstanding educational atmosphere, striking the perfect equilibrium between systematic structure and creative freedom. The 12-week intensive curriculum allowed for gradual advancement from foundational AWS principles to implementing enterprise-grade solutions. Weekly worklog maintenance facilitated organized task management and accurate progress evaluation. This environment championed self-directed exploration while ensuring resources and assistance remained readily accessible.\n2. Support from Mentor / Team\nThe mentorship caliber and collaborative spirit throughout the program proved truly impressive. Every team member enthusiastically engaged in conversations about technical obstacles, analyzed my ideas, and navigated me through challenging AWS concepts. What resonated most deeply was the mentor\u0026rsquo;s approach in stimulating independent investigation and original solution development before offering specific direction.\n3. Relevance of Work to Academic Major\nThe training content demonstrated strong alignment with contemporary cloud computing trends and advanced software engineering processes. The learning trajectory began with networking foundations (VPC, Security Groups) and gradually advanced toward topics including serverless architectures, databases, security, and AI/ML services, helping establish robust cloud engineering capabilities. The application project—constructing an AI-powered chatbot alongside admin dashboard—created opportunities to accumulate practical battle-tested experience applicable in genuine work environments. The concentration on Infrastructure as Code (CDK) and DevOps practices resonated perfectly with current industrial requirements.\n4. Learning \u0026amp; Skill Development Opportunities\nThe 12-week process unveiled valuable training opportunities across diverse domains:\nTechnical Capabilities: Accumulated direct experience with 10+ AWS services, CDK, serverless computing, database administration, and premier security standards Architecture Planning: Cultivated the ability to construct scalable, cost-efficient cloud solutions through draw.io and Q Developer CLI Documentation Skills: Refined technical content writing capabilities via periodic worklog creation and translating 3 AWS articles Situation Handling: Advanced debugging abilities through resolving Lambda VPC conflicts, timeout challenges, and checksum optimization Cost Management: Mastered methods for making architecture decisions based on cost-benefit analysis (S3 archiving strategies) The learning pathway from theoretical foundation (Weeks 1-3) → security \u0026amp; databases (Weeks 4-5) → architecture design (Weeks 6-9) → full deployment (Weeks 10-12) was arranged scientifically and coherently.\n5. Program Culture \u0026amp; Team Spirit\nThe FCJ program cultivated an environment of persistent learning, cooperation, and innovation. Team sessions proceeded with high constructiveness featuring open exchanges about technical choices and architectural strategies. The \u0026ldquo;Data Science on AWS\u0026rdquo; workshop during week 6 along with numerous learning activities opened avenues to expand understanding beyond the core program. The team collectively celebrated achieved milestones (passing midterm examination, completing project proposal, successful deployments) while preserving a continuous learning mindset. Members actively disseminated knowledge via Notion documentation and collaboratively addressed difficulties.\n6. Program Policies / Support\nThe program was established with professional structure, weekly objectives clearly outlined, abundant reference resources (AWS Study Group, Cloud Journey, official AWS documentation) accompanied by application labs. The freedom to experiment with various approaches (exploring File Storage Gateway, FSx, diverse database options) while maintaining focus on the primary project proved genuinely valuable. Utilizing AWS Free Tier accounts coupled with guidance on budget administration helped grasp cost optimization methods from the initial days.\nDetailed Feedback on Program Phases Phase 1 (Weeks 1-3): AWS Fundamentals \u0026amp; Core Services\nStrengths: Systematic introduction covering IAM, networking, compute, and storage comprehensively. Hands-on labs reinforced theoretical knowledge effectively. Phase 2 (Weeks 4-5): Security, Identity \u0026amp; Databases\nStrengths: Excellent progression into advanced IAM concepts, KMS encryption, and database fundamentals. Translation exercises enhanced technical English proficiency. Phase 3 (Weeks 6-9): Architecture Design \u0026amp; Project Planning\nStrengths: Learning architecture design tools (draw.io, Q Developer CLI) and studying AWS Solutions Library architectures provided practical context. The midterm exam validated knowledge comprehensively. Phase 4 (Weeks 10-12): Full-Stack Deployment \u0026amp; Infrastructure as Code\nStrengths: Hands-on CDK deployment experience proved invaluable. Iterative refinement through multiple deployments taught real-world DevOps practices. What I Found Most Satisfying Delivering a Production-Ready Solution: Successfully deployed a complete chatbot system from scratch, including admin dashboard, Messenger integration, and automated data archiving Cost Optimization Achievements: Proposed and implemented cost-saving measures (DynamoDB cache, checksum mechanism) that significantly reduced projected expenses Problem-Solving Victories: Overcame technical challenges such as Lambda VPC limitations, timeout issues with Vietnamese prompts, and data synchronization logic Documentation \u0026amp; Knowledge Sharing: Built comprehensive worklogs across 12 weeks and translated AWS blog posts, improving technical communication skills Measurable Progress: Transformed from an AWS beginner in Week 1 to confidently deploying multi-service architectures by Week 12 Would I Recommend This Program? Absolutely! I highly recommend the First Cloud Journey program to anyone interested in cloud computing for several reasons:\nComprehensive Curriculum: Covers all essential AWS service categories from basics to advanced topics Hands-On Approach: Real project deployment provides practical experience beyond certifications Structured Progression: 12-week timeline with clear milestones helps maintain momentum Supportive Community: Mentors and team members actively assist with technical challenges Career Relevance: Skills acquired (CDK, serverless, security, cost optimization) are directly applicable to industry roles Flexibility: Program allows exploring different services while focusing on core objectives The program truly transformed my understanding from theoretical knowledge to practical implementation capabilities.\nSuggestions \u0026amp; Expectations for Future Iterations Suggestions to Improve the Internship Experience:\nEnhanced Pre-Internship Preparation\nProvide a pre-reading list or video series covering basic cloud concepts Set up AWS accounts and CLI before Week 1 to maximize hands-on time Weekly Retrospectives\nStructured reflection sessions each week to discuss challenges and learnings Share common issues and solutions across all interns Guest Speaker Sessions\nInvite AWS Solutions Architects or cloud professionals to share real-world experiences Industry insights on emerging services and best practices Capstone Presentation\nFormal presentation of final projects to mentors and peers Practice communicating technical decisions to various audiences Alumni Network\nConnect with previous FCJ program participants Mentorship opportunities for future interns Would I Continue This Program in the Future?\nYes, I would love to continue with advanced programs such as:\nAdvanced AWS Specializations: Machine Learning, Data Analytics, or Security specialty tracks AWS Certification Preparation: Solutions Architect Professional, DevOps Engineer Open-Source Contributions: Contributing to AWS CDK constructs or community projects Additional Comments:\nThe First Cloud Journey program exceeded my expectations in every aspect. The 12-week journey from AWS fundamentals to deploying a complete AI-powered chatbot solution significantly boosted my confidence in cloud engineering abilities. The emphasis on hands-on learning, cost optimization, and best practices prepared me well for a career in cloud computing.\nWhat I appreciate most about the program:\nFreedom to explore different architectural approaches and learn from mistakes Supportive team environment that encouraged questions and creative solutions Comprehensive documentation requirements that considerably improved my technical writing skills Real-world project requiring integration of multiple AWS services Thank you to the FCJ team, mentors, and fellow interns for an incredible learning experience. The knowledge and skills gained during these 12 weeks will serve as a strong foundation for my future career in cloud computing and software engineering.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.07-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn how to draw and design AWS architecture using draw.io following best practices. Continue developing and refining pre-architecture for chatbot project. Define direction for project, including main features and use cases. Research Text-to-SQL mechanism and how to apply LLM to convert natural language to SQL queries. Analyze database security issues when chatbot interacts directly with DB (permission limits, access control). Review foundational AWS knowledge to prepare for exam. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Learn how to draw AWS architecture through tutorial video: + Use draw.io with AWS icons + Best practices in designing architecture diagrams + How to show flow and connections between services - Note incident: AWS outage at us-east-1, severely affecting many global systems 20/10/2025 20/10/2025 AWS Study Group 3 - Continue drawing and editing architecture for project: + Based on brainstormed pre-architecture + Add new components + Optimize flow and connections - Team meeting on project direction: + Define main chatbot objectives + Define scope and features + Analyze use cases 21/10/2025 21/10/2025 Notion - Pre Architecture Notion - Directions 4 - Research Text-to-SQL mechanism: + Read paper: \u0026ldquo;HEXGEN-FLOW: Optimizing LLM Inference Request Scheduling for Agentic Text-to-SQL\u0026rdquo; + Understand how LLM converts natural language to SQL queries + Analyze processing flow and optimization techniques - Team meeting on chatbot interaction with database: + Clarify how chatbot processes Text-to-SQL + Limit chatbot permissions (READ-ONLY vs READ-WRITE) + Secure sensitive information + Control dangerous commands (DROP, DELETE, UPDATE) 22/10/2025 22/10/2025 Paper - HEXGEN-FLOW Notion Meeting Notes 5 - Review foundational AWS knowledge: + Review learned services (EC2, RDS, S3, Lambda, etc.) + Review best practices and use cases + Prepare for periodic exam 23/10/2025 23/10/2025 Cloud Journey - AWS Study Group Week 7 Achievements: Mastered how to draw AWS architecture using draw.io\nImproved pre-architecture for chatbot project\nClearly defined direction and scope for project:\nMain chatbot objectives and features Specific use cases Initial deployment phase scope Deep understanding of Text-to-SQL mechanism:\nHow LLM converts natural language to SQL queries Processing flow and optimization techniques from HEXGEN-FLOW paper Challenges in parsing and generating accurate SQL Detailed analysis of database security issues when chatbot interacts with DB:\nDetermined need to limit chatbot permissions (READ-ONLY or allow WRITE) How to control dangerous commands (invalid DROP, DELETE, UPDATE) Protect sensitive information and prevent SQL injection Design appropriate permission model Noted and tracked AWS outage incident at us-east-1\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.8-setupadmindashboard/","title":"Setup Admin Dashboard","tags":[],"description":"","content":"Setup Admin Dashboard The Admin Dashboard requires an administrator account created in Amazon Cognito. Follow these steps to access the dashboard for the first time.\nStep 1: Create an Admin User Run the following AWS CLI command to create an admin account:\naws cognito-idp admin-create-user --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --username \u0026lt;your-email\u0026gt; --user-attributes Name=email,Value=\u0026lt;your-email\u0026gt; Name=email_verified,Value=true --temporary-password \u0026#34;\u0026lt;your-temporary-password\u0026gt;\u0026#34; --region ap-northeast-1 Both the User Pool ID and the Admin Dashboard URL can be found in the outputs.json file generated after CDK deployment.\nStep 2: First Login Open the Admin Dashboard URL from the outputs.json file Log in with your email and temporary password Cognito will prompt you to set a new permanent password After updating your password, you will be redirected to the Admin Dashboard Step 3: Verify Access After logging in, you should see the dashboard homepage with:\nSystem statistics (total customers, consultants, appointments) Appointments breakdown by status Recent appointments list You can create additional admin users by repeating Step 1 with different email addresses.\nTroubleshooting Issue: Cannot login\nVerify the User Pool ID is correct Check if the user was created successfully in Cognito Console Ensure you\u0026rsquo;re using the correct temporary password Now you\u0026rsquo;re ready to use the Admin Dashboard to manage consultants and appointments!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.08-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS service knowledge to prepare for the midterm exam. Practice quizzes and sample exams to reinforce theory and practical skills. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Continue reviewing AWS services for the midterm exam 27/10/2025 27/10/2025 3 - Continue reviewing AWS services for the midterm exam 28/10/2025 28/10/2025 4 - Review theory via Quizlet: AWS Hotfix v1.0 29/10/2025 29/10/2025 Quizlet 5 - Practice sample exams from AWS Cloud Practitioner Practice Exam 30/10/2025 30/10/2025 GitHub Practice Exam 6 - Take the midterm exam 31/10/2025 31/10/2025 AWS FCJ Midterm Test Week 8 Achievements: Comprehensive review of AWS service groups: Compute, Storage, Networking, Database, Security, Monitoring, etc. Reinforced knowledge through quizzes, mock exams, and practice questions. Familiarity with real exam structure and question formats. Completed the First Cloud Journey midterm exam. "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.9-usingchatbot/","title":"Using the Chatbot","tags":[],"description":"","content":"Using the Chatbot The MeetAssist chatbot can be accessed via Facebook Messenger and provides a conversational interface for booking appointments with consultants.\nGetting Started Open Facebook Messenger and search for your connected page Click the Get Started button The chatbot will greet you and request authentication Authentication Flow Before using the chatbot, you need to authenticate:\nEmail Verification: Enter your registered email address OTP Request: The bot will send a One-Time Password to your email Enter OTP: Enter the 6-digit code from your email (check spam folder) Active Session: Your session is valid for 24 hours If your AWS SES is in sandbox mode, you can only send emails to verified addresses. For production use, request SES production access.\nBooking an Appointment The chatbot uses natural language understanding capabilities powered by Amazon Bedrock. You can book appointments using conversational Vietnamese:\nExample conversations:\n\u0026ldquo;Tôi muốn đặt lịch với tư vấn viên Nguyễn Văn A vào thứ 2 tuần sau lúc 10 giờ sáng\u0026rdquo; \u0026ldquo;Book appointment with consultant John next Monday at 2pm\u0026rdquo; \u0026ldquo;Đặt lịch gặp chuyên gia Trần Thị B ngày mai buổi chiều\u0026rdquo; The chatbot will:\nExtract appointment information (consultant, date, time) Check the consultant\u0026rsquo;s available schedule Confirm the booking with you Create the appointment in the system Updating an Appointment To edit an existing appointment:\n\u0026ldquo;Tôi muốn đổi lịch hẹn sang thứ 4\u0026rdquo; \u0026ldquo;Change my appointment to 3pm\u0026rdquo; \u0026ldquo;Reschedule my meeting with consultant A\u0026rdquo; The chatbot will display your current appointments and guide you through the update process.\nCanceling an Appointment To cancel an appointment:\n\u0026ldquo;Hủy lịch hẹn của tôi\u0026rdquo; \u0026ldquo;Cancel my appointment\u0026rdquo; \u0026ldquo;I want to cancel my meeting\u0026rdquo; The bot will list your active appointments and request confirmation before canceling.\nGeneral Queries Ask the chatbot about:\nConsultants:\n\u0026ldquo;Có những tư vấn viên nào?\u0026rdquo; \u0026ldquo;Who are the available consultants?\u0026rdquo; \u0026ldquo;Tell me about consultant Nguyễn Văn A\u0026rdquo; Available Schedule:\n\u0026ldquo;Tư vấn viên A có lịch trống khi nào?\u0026rdquo; \u0026ldquo;Show me available slots for next week\u0026rdquo; \u0026ldquo;When is consultant B free?\u0026rdquo; Your Appointments:\n\u0026ldquo;Lịch hẹn của tôi\u0026rdquo; \u0026ldquo;My appointments\u0026rdquo; \u0026ldquo;Show my upcoming meetings\u0026rdquo; Aborting Actions If you want to cancel the current conversation flow:\nType \u0026ldquo;abort\u0026rdquo; or \u0026ldquo;hủy\u0026rdquo; at any time The bot will reset and you can start a new request Session Management Sessions expire after 24 hours You will need to re-authenticate with email + OTP Your conversation history is maintained throughout the session Troubleshooting Issue: Bot not responding\nCheck if the Facebook webhook is configured correctly Verify the API Gateway URL in webhook settings Check Lambda logs in CloudWatch Issue: Not receiving OTP\nVerify your email is correct Check spam/junk folder If in SES sandbox mode, ensure the email is verified in SES Console Issue: Appointment booking fails\nEnsure the consultant exists in the database Check if the requested time slot is available Verify the date format is understood by the bot Issue: Bedrock model throttling\nIf you see slow responses, you may be hitting Bedrock rate limits Consider requesting quota increases in Service Quotas console The chatbot learns from context. If it doesn\u0026rsquo;t understand, try rephrasing your request with more specific details like exact dates and times.\nNow you can start interacting with the MeetAssist chatbot to manage your appointments!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.09-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Finalize and approve overall architecture of chatbot project. Build detailed proposal for the project. Collect sample data and research dashboard interfaces. Tasks to carry out this week: Day Tasks Start date Completion date References 3 - Meeting to approve overall chatbot project architecture and preliminary proposal - Collect sample data and research various dashboard interfaces to leverage ideas for project\u0026rsquo;s admin dashboard section 04/11/2025 04/11/2025 4 - Create first version of project proposal (draft) 05/11/2025 05/11/2025 5 - Team members review draft proposal and make revisions 06/11/2025 06/11/2025 6 - Finalize proposal and submit 07/11/2025 07/11/2025 Week 9 Achievements: Successfully completed and got approval for overall chatbot project architecture.\nCollected sample data and researched dashboard interfaces for project reference.\nCompleted first draft version of project proposal.\nAll team members reviewed and contributed feedback for proposal revisions.\nFinalized and submitted project proposal on deadline.\nHave clear vision of project direction and implementation scope for chatbot project.\n"},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.10-usingadmindashboard/","title":"Using the Admin Dashboard","tags":[],"description":"","content":"Using the Admin Dashboard The Admin Dashboard provides a comprehensive interface for managing consultants, schedules, and appointments.\nOverview Page After logging in, the dashboard homepage displays:\nSystem Statistics:\nTotal number of customers Total number of consultants Total number of appointments Appointments Breakdown:\nVisual chart showing appointments by status: Pending Confirmed Completed Cancelled Recent Appointments:\nList of the most recent appointments Quick view of customer, consultant, date, and status Consultants Management Navigate to the Consultants section to manage your consultant team.\nView Consultants See all consultants in a table view Columns: Name, Email, Specialization, Account Status Add a New Consultant Click the Add Consultant button Fill in the consultant details: Full Name Email Phone Number Specialization Qualifications Click Create Creating a consultant will automatically create a corresponding Cognito user account with temporary credentials sent to their email.\nEdit Consultant Information Click the Edit button next to a consultant Update the desired fields Click Save Changes Delete a Consultant Click the Delete button next to a consultant Confirm the deletion The consultant\u0026rsquo;s Cognito account will also be removed Reset Consultant Password Click the Reset Password button A new temporary password will be generated The consultant will receive the password via email Appointments Management Navigate to the Appointments section for comprehensive appointment oversight.\nView Appointments Table view of all appointments Filter by: Status (Pending, Confirmed, Completed, Cancelled) Date range Consultant Customer Create Manual Appointment Click Create Appointment Select customer (or create new) Select consultant Choose date and time Add notes (optional) Click Book Appointment Update Appointment Click Edit on an appointment Modify details (time, consultant, notes) Click Update Change Appointment Status Manually update appointment status:\nPending → Confirmed: Approve a booking Confirmed → Completed: Mark as finished Any → Cancelled: Cancel the appointment You now have full control over the MeetAssist system through the Admin Dashboard!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Deploy admin dashboard interface for chatbot project. Deploy static website on AWS S3 and CloudFront. Integrate Route 53 and CloudFront to serve website. Research and prepare to integrate Amazon Cognito for admin login functionality. Tasks to carry out this week: Day Tasks Start date Completion date References 2 - Deploy first step of admin flow, design demo dashboard interface 10/11/2025 10/11/2025 3 - Push frontend code files to Amazon S3, test S3 static hosting - Successfully deploy static web using S3 11/11/2025 11/11/2025 Amazon S3 Static Website Hosting 4 - Research CloudFront to integrate with previously deployed static web - Configure CloudFront distribution to integrate with S3 static web 12/11/2025 12/11/2025 Amazon CloudFront Documentation 5 - Successfully deploy static web on S3 via CloudFront and Route 53 13/11/2025 13/11/2025 CloudFront with S3 6 - Meeting to discuss and revise project architecture - Remove Route 53 from architecture, use CloudFront domain directly - Research to integrate admin login functionality using Cognito 14/11/2025 14/11/2025 Amazon Cognito Documentation Week 10 Achievements: Completed design of demo dashboard interface for admin flow.\nSuccessfully deployed static website hosting on Amazon S3:\nUploaded frontend code to S3 bucket Configured S3 bucket for static website hosting Tested and verified website works correctly Integrated CloudFront with S3 static website:\nCreated and configured CloudFront distribution Connected CloudFront with S3 bucket Optimized performance and caching Fully deployed static web via CloudFront and Route 53:\nConfigured DNS with Route 53 Connected domain with CloudFront distribution Tested and verified entire flow Adjusted project architecture:\nDecided to remove Route 53 from architecture Use CloudFront domain directly to simplify Updated and optimized architecture diagram Researched and prepared to integrate Amazon Cognito:\nLearned about Cognito User Pool Researched authentication flow for admin dashboard Planned login functionality implementation "},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Implement core components and pages for Admin Dashboard. Build basic UI components using React and Tailwind CSS. Set up application structure with routing and API services. Configure backend stack and improve security for admin dashboard. Design new database schema and prepare sample data. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Implement core UI components: Button, Card, StatCard, Header, Modal, Sidebar - Style application using Tailwind CSS and custom styles - Set up main application structure with routing 17/11/2025 17/11/2025 3 - Add Analytics, Conversations, Crawler, and Overview pages - Implement services for API interactions - Define TypeScript types to ensure type safety - Configure Vite for development and build 18/11/2025 18/11/2025 4 - Remove unnecessary files and duplicate code in admin_stack - Refactor configuration handling and improve security policies - Add AdminBackendStack to manage backend resources and API 19/11/2025 19/11/2025 5 - Update Content Security Policy (CSP) to allow Cognito connection - Add deployment scripts and watch-sync functionality for S3 - Test database connection with CRUD operations 20/11/2025 20/11/2025 6 - Improve OAuth callback handling and CSP settings - Enhance error handling for Cognito OAuth callback - Adjust Cognito callback URLs and logout URLs - Design new database schema and prepare sample data - Update Appointments, Consultants, Programs management pages with CRUD functionality 21/11/2025 21/11/2025 Week 11 Achievements: Completed implementation of basic UI components:\nButton with loading state and icon support Card and StatCard for displaying metrics Header component for page titles and actions Reusable Modal component Sidebar with navigation menu and user profile Successfully built admin dashboard application structure:\nSet up routing for different pages Created pages: Overview, Analytics, Conversations, Crawler Styling with Tailwind CSS Configured Vite build tool Implemented API services layer:\nAnalyticsService for data analysis ConversationService for conversation management CrawlerService for crawler operations Type safety with TypeScript interfaces Improved infrastructure and security:\nRefactored admin_stack, removed duplicate code Added AdminBackendStack for backend resources Updated Content Security Policy Integrated Amazon Cognito authentication Completed OAuth integration:\nImproved OAuth callback handling Enhanced error handling Configured callback and logout URLs Tested and verified authentication flow Database and data management:\nDesigned new database schema Prepared sample data Tested CRUD operations Implemented management pages: Appointments, Consultants, Programs with full CRUD functionality Deployment and automation:\nAdded deployment scripts Set up watch-sync for S3 Optimized development workflow "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.11-usingconsultantportal/","title":"Using the Consultant Portal","tags":[],"description":"","content":"Using the Consultant Portal The Consultant Portal is a dedicated interface for consultants to manage their appointments and view their schedules.\nAccessing the Consultant Portal Open the Consultant Portal URL from the outputs.json file or URL from FrontendStack Outputs in the AWS CloudFormation console. Log in with your consultant email and password First-time users will need to change their temporary password Consultant accounts are created by administrators through the Admin Dashboard. You will receive your login credentials via email.\nMy Appointments Page The My Appointments page is your central hub for managing all your scheduled meetings.\nView Appointments See all your appointments in a table view with:\nCustomer name and contact Appointment date and time Status (Pending, Confirmed, Completed, Cancelled) Filter Appointments Use the filter options to narrow down your view:\nBy Status: Show only pending, confirmed, or completed appointments By Date Range: View appointments for specific time periods Appointment Actions For each appointment, you can:\nConfirm Appointments:\nChange status from Pending to Confirmed Customer receives automatic email notification Complete Appointments:\nMark appointments as Completed after the meeting Cancel Appointments:\nCancel appointments with a reason System sends cancellation email to customer You cannot directly modify appointment times or dates. Contact your administrator if changes are needed.\nMy Schedule Page The My Schedule page displays your weekly availability.\nWeekly View See your time slots organized by day of week Color-coded slot status: Green: Available Gray: Booked/Unavailable View-Only Schedule Consultants have read-only access to their schedules. All schedule modifications must be made by administrators through the Admin Dashboard.\nTroubleshooting Issue: Cannot log in\nVerify you\u0026rsquo;re using the correct Consultant Portal URL (not Admin Dashboard) Reset password if needed You\u0026rsquo;re now ready to effectively manage your consulting appointments through the Consultant Portal!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Implement theme switching support (light/dark) for admin dashboard. Refactor AWS configuration and database schema. Improve UI/UX with advanced components. Implement consultant schedule management functionality. Optimize infrastructure with custom resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Implement theme switching support (light/dark mode) - Refactor pages to use theme styles - Enhance UI components with loading indicators and responsive design - Improve accessibility and usability of forms and buttons 24/11/2025 24/11/2025 3 - Refactor AWS configuration: migrate from .env to config.json - Load configuration from /config.json (auto-generated by CDK FrontendStack) - Remove environment variables and fallback mechanisms - Delete obsolete deploy.sh script 25/11/2025 25/11/2025 4 - Modify database schema (remove CommunityProgram table) - Delete ProgramsPage component and API calls - Remove functions related to community program - Update OverviewPage and Sidebar - Replace icons with lucide-react - Add recharts dependencies and enhance OverviewPage 26/11/2025 26/11/2025 5 - Refactor archive service: remove unused CSV mappings - Implement custom resource to create config.json from SSM parameters - Upload config.json to S3 27/11/2025 27/11/2025 6 - Add API functions for consultant schedule management: + getConsultantSchedules + getScheduleByConsultant + createConsultantSchedule + updateConsultantSchedule + deleteConsultantSchedule + generateConsultantSchedule - Update Admin service to handle schedule actions - Enhance appointment retrieval with filters - Add ConsultantSchedule type and error handling 28/11/2025 28/11/2025 Week 12 Achievements: Completed theme switching implementation:\nLight/dark mode Refactored all pages to use theme styles Consultants, Conversations, Overview, Programs pages Improved loading indicators and responsive design Refactored AWS configuration:\nMigrated from .env to config.json Auto-generated config by CDK FrontendStack Removed environment variables Deleted old deploy.sh script Load config from /config.json endpoint Database schema optimization:\nRemoved unused CommunityProgram table Removed ProgramsPage component Cleaned up API calls and admin service functions Updated OverviewPage and Sidebar Removed CommunityProgram interface UI/UX improvements:\nReplaced icons with lucide-react Added dependencies: lucide-react and recharts Enhanced OverviewPage with charts and stat cards Modern and consistent design Improved accessibility and usability Better confirmation dialogs with globalThis.confirm Infrastructure optimization:\nRefactored archive service Removed unused CSV mappings Implemented custom resource for config.json Auto-generated config from SSM parameters Uploaded config to S3 Consultant schedule management:\nImplemented full CRUD operations for consultant schedules Auto-generated schedules Filtered appointments by consultant and customer ConsultantSchedule type for type safety Enhanced error handling and logging Integration with Admin service Code quality improvements:\nBetter type safety with TypeScript Improved error handling Enhanced logging mechanisms Code cleanup and organization Consistent coding patterns "},{"uri":"https://rinnn465.github.io/aws-workshop/5-workshop/5.12-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources To avoid incurring unnecessary AWS charges, follow these steps to completely remove all resources created during this workshop.\nThis cleanup process is irreversible. All data, including customer information, appointments, and consultant records will be permanently deleted. Make sure to export any data you need before proceeding.\nStep 1: Destroy CDK Stack Navigate to your project directory and destroy the CDK stack:\ncd MeetAssist cdk destroy --all When prompted, confirm the deletion by typing y.\nThis will remove:\nLambda functions API Gateway RDS PostgreSQL database DynamoDB tables Cognito User Pools S3 buckets (except those with versioning/retention) CloudFront distributions SQS queues EventBridge rules IAM roles and policies The CDK destroy process may take 30-40 minutes to complete. Wait for confirmation before proceeding to the next step.\nStep 2: Delete S3 Buckets Some S3 buckets may not be automatically deleted if they contain objects. Manually delete them:\nList your buckets:\naws s3 ls | grep meetassist Empty and delete each bucket:\naws s3 rm s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 aws s3 rm s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 Replace \u0026lt;account-id\u0026gt; with your actual AWS account ID.\nStep 3: Remove Cognito Users If you manually created any Cognito users that weren\u0026rsquo;t deleted:\naws cognito-idp list-users --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --region ap-northeast-1 The CDK destroy should have removed the User Pool, but verify in the console that no orphaned resources remain.\nStep 4: Delete Secrets Manager Secrets Check for any remaining secrets:\naws secretsmanager list-secrets --region ap-northeast-1 | grep meetassist Delete any found secrets:\naws secretsmanager delete-secret --secret-id MeetAssist/Facebook/PageAccessToken --region ap-northeast-1 --force-delete-without-recovery aws secretsmanager delete-secret --secret-id MeetAssist/Facebook/VerifyToken --region ap-northeast-1 --force-delete-without-recovery Step 5: Delete SSM Parameters Remove the Facebook App ID and App Secret:\naws ssm delete-parameter --name /MeetAssist/Facebook/AppId --region ap-northeast-1 aws ssm delete-parameter --name /MeetAssist/Facebook/AppSecret --region ap-northeast-1 Step 6: Remove Facebook App Configuration Go to Facebook Developers Navigate to your MeetAssist app Go to Settings → Basic Scroll down and click Delete App Confirm the deletion Alternatively, you can keep the Facebook App but remove the webhook subscription and page connection if you plan to rebuild the project later.\nStep 7: Disable Bedrock Models (Optional) If you no longer need access to the Bedrock models:\nGo to AWS Console → Amazon Bedrock Navigate to Model access in the left sidebar For each enabled model: Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku Titan Embeddings G1 - Text Click Manage → Disable access Bedrock model access itself doesn\u0026rsquo;t incur charges. You\u0026rsquo;re only billed for API calls. You can leave the models enabled if you plan to use them in other projects.\nStep 8: Verify Cleanup Double-check that all resources are removed:\nCheck CloudFormation Stacks:\naws cloudformation list-stacks --region ap-northeast-1 | grep MeetAssist Check Lambda Functions:\naws lambda list-functions --region ap-northeast-1 | grep MeetAssist Check RDS Instances:\naws rds describe-db-instances --region ap-northeast-1 | grep meetassist Check DynamoDB Tables:\naws dynamodb list-tables --region ap-northeast-1 | grep MeetAssist All commands should return empty results.\nCost Considerations After cleanup, you should see the following in your AWS billing:\nOngoing Charges (None):\nLambda, API Gateway, RDS, DynamoDB - $0 (resources deleted) CloudFront - $0 (distribution deleted) S3 storage - $0 (buckets emptied and deleted) One-time Charges:\nBedrock API calls - Based on usage during workshop Data transfer costs - Minimal SES emails sent - Negligible cost Monitor your AWS Cost Explorer for 2-3 days after cleanup to ensure no unexpected charges appear.\nTroubleshooting Cleanup Issues Issue: CDK destroy fails\nCheck CloudFormation console for specific error messages Manually delete stuck resources through AWS Console Retry cdk destroy after manual intervention Issue: RDS instance not deleting\nCheck if deletion protection is enabled Disable in RDS Console → Modify → uncheck \u0026ldquo;Enable deletion protection\u0026rdquo; Create final snapshot or skip if not needed Issue: Still seeing charges\nCheck for resources in other regions (this workshop uses ap-northeast-1) Look for CloudWatch Logs log groups (can accumulate storage costs) Review Cost Explorer for detailed breakdown Congratulations! You\u0026rsquo;ve successfully cleaned up all workshop resources. Thank you for completing this workshop!\n"},{"uri":"https://rinnn465.github.io/aws-workshop/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Implement Consultant Portal with authentication and schedule/appointment management. Integrate Cognito User Pool for consultants. Clean up code and refactor project structure. Implement customer management and email notification system. Improve user experience with multilingual support. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create Consultant Portal UI - Integrate with Cognito Consultant User Pool - Fix authentication and API related bugs 01/12/2025 01/12/2025 3 - Add Login page with Cognito OAuth2 - Create Schedule page with calendar navigation - Create Appointments page with confirm/deny functionality - Implement responsive UI with Tailwind CSS - Add backend functions: get_my_schedule, get_my_appointments, confirm/deny_appointment - Fix CORS errors, API paths, and account status display 02/12/2025 02/12/2025 4 - Code cleanup: remove unused SSM parameters - Delete unnecessary API functions - Remove watch-sync scripts - Refactor: rename admin-dashboard → frontend - Organize file structure by roles (admin, consultant) 03/12/2025 03/12/2025 5 - Translate UI text to Vietnamese for multiple pages - Implement modal confirmation for appointment actions - Enhance error handling in API service - Customize email templates for Consultant User Pool 04/12/2025 04/12/2025 6 - Implement customer management with CRUD operations - Remove unnecessary fields from customer management form - Add Lambda Email Notification (outside VPC) - Integrate SES for email confirmation/cancellation - Extend API service with email notification methods 05/12/2025 05/12/2025 Week 13 Achievements: Successfully implemented Consultant Portal:\nNew user interface for consultants Cognito authentication integration with Consultant User Pool Login page with OAuth2 flow Responsive UI with Tailwind CSS Functional pages for Consultant Portal:\nSchedule page: view weekly schedule with calendar navigation Appointments page: view, confirm and deny appointments Personal schedule management Backend APIs for Consultant Portal:\nget_my_schedule: consultants view their own schedules get_my_appointments: view appointments confirm_appointment and deny_appointment: handle appointments get_consultant_by_email: map Cognito user to consultant_id Bug fixes and improvements:\nFixed CORS errors (removed allow_credentials) Switched to using idToken instead of accessToken Fixed API paths and SQL queries Improved account status display (Active/Pending) Enhanced error handling Code cleanup and refactoring:\nRemoved unused SSM parameters Deleted unused API functions: getTables, getTableSchema, getDatabaseStats Removed watch-sync scripts Cleaned up redundant comments (TODO, BugNote) Renamed admin-dashboard → frontend Organized file structure by roles (admin/consultant) Localization (Multilingual support):\nTranslated UI text to Vietnamese ConsultantsPage, OverviewPage, AppointmentsPage, SchedulePage App services and error messages Customer management:\nFull CRUD operations for customer management UI integration with admin dashboard Simplified forms (removed unnecessary fields) Email notification system:\nLambda Email Notification function (outside VPC) Amazon SES integration Auto-send emails for appointment confirmation/cancellation Backend returns email details to frontend Frontend triggers email notifications Email templates customization:\nConsultant User Pool email templates Account creation by admin messages Account verification emails Appointment confirmation/cancellation emails Improved user experience:\nModal confirmations for critical actions Better error handling and user feedback Consistent Vietnamese/English UI Streamlined workflows "},{"uri":"https://rinnn465.github.io/aws-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://rinnn465.github.io/aws-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]